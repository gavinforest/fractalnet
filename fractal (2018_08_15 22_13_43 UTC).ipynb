{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from deap import gp\n",
    "import operator\n",
    "import math\n",
    "def square(x):\n",
    "    return x*x\n",
    "def absSqrt(x):\n",
    "    return math.sqrt(abs(x))\n",
    "pset = gp.PrimitiveSet(\"nodePrims\", arity=1)\n",
    "pset.addPrimitive(max, 2)\n",
    "pset.addPrimitive(operator.mul, 2)\n",
    "pset.addPrimitive(operator.add, 2)\n",
    "pset.addPrimitive(operator.sub, 2)\n",
    "pset.addPrimitive(math.sin, 1)\n",
    "pset.addPrimitive(math.cos, 1)\n",
    "pset.addPrimitive(absSqrt, 1)\n",
    "pset.addPrimitive(square, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gen\tnevals\tmean   \tmin    \tmax    \tstdDev \n",
      "0  \t50    \t12.0987\t1.67842\t233.558\t33.8616\n",
      "1  \t23    \t27.0639\t4      \t233.558\t49.6745\n",
      "2  \t27    \t52.7762\t7.90471\t233.558\t63.3216\n",
      "3  \t20    \t92.0165\t11.5403\t233.558\t77.1398\n",
      "4  \t24    \t176.354\t57.5442\t233.558\t82.4405\n",
      "5  \t23    \t229.947\t57.5442\t265.123\t28.0427\n",
      "6  \t27    \t265.12 \t233.558\t467.116\t76.7934\n",
      "7  \t25    \t500.791\t233.558\t6561   \t976.817\n",
      "8  \t25    \t1212.91\t233.558\t6561   \t2022.16\n",
      "9  \t23    \t3521.36\t467.116\t6561   \t3039.97\n",
      "10 \t22    \t6136.84\t759.352\t6780.24\t1531.97\n",
      "11 \t23    \t6588.4 \t6561   \t6780.24\t72.5055\n",
      "12 \t23    \t6643.21\t6561   \t6780.24\t106.137\n",
      "13 \t27    \t6725.91\t6561   \t6780.24\t90.0939\n",
      "14 \t28    \t6785.67\t6736.53\t6910.72\t29.4875\n",
      "15 \t24    \t6812.86\t6780.24\t6910.72\t56.4995\n",
      "16 \t22    \t6858.52\t6780.24\t6910.72\t63.9219\n",
      "17 \t23    \t6910.72\t6910.72\t6910.72\t9.09495e-13\n",
      "18 \t24    \t6910.72\t6910.72\t6910.72\t9.09495e-13\n",
      "19 \t21    \t6984.45\t6910.72\t7648.05\t221.2      \n",
      "20 \t21    \t7095.05\t6910.72\t7648.05\t319.275    \n",
      "21 \t18    \t7445.28\t6910.72\t7648.05\t329.231    \n",
      "22 \t24    \t15302.3\t7648.05\t68881.6\t20251.1    \n",
      "23 \t24    \t30621.9\t7648.05\t68881.6\t29635.9    \n",
      "24 \t25    \t59613  \t7648.05\t68881.6\t21835.9    \n",
      "25 \t25    \t68881.6\t68881.6\t68881.6\t0          \n"
     ]
    }
   ],
   "source": [
    "from deap import creator, base, tools, algorithms\n",
    "from random import randint\n",
    "import numpy as np\n",
    "import pickle\n",
    "import itertools\n",
    "DataDims = 2\n",
    "MINDIMFUNCS = 1\n",
    "MAXDIMFUNCS = 10\n",
    "MINDIMS = DataDims\n",
    "MAXDIMS = 10\n",
    "POPSIZE = 50\n",
    "GENERATIONS = 25\n",
    "MU = 40\n",
    "LAMBDA = 30\n",
    "CXPB = 0.5\n",
    "MUTPB = 0.3\n",
    "\n",
    "\n",
    "def genTreeList():\n",
    "    return [gp.PrimitiveTree(gp.genHalfAndHalf(pset=pset, min_=1,max_=6)) for i in range(randint(MINDIMFUNCS,MAXDIMFUNCS))]\n",
    "def genFuncList():\n",
    "    return [genTreeList() for j in range(randint(MINDIMS,MAXDIMS))]\n",
    "\n",
    "creator.create(\"FitnessMax\", base.Fitness, weights=(1.0,) )\n",
    "creator.create(\"Individual\", list, fitness=creator.FitnessMax)\n",
    "\n",
    "# def myMutate(individual):\n",
    "#     new = [[gp.mutNodeReplacement(x,pset=pset) for x in l] for l in toolbox.clone(individual)]\n",
    "#     return (tools.initIterate(creator.Individual, lambda: new),)\n",
    "\n",
    "\n",
    "def getFitMap(ls):\n",
    "    fitnesses = []\n",
    "    for l in ls:\n",
    "        fitnesses.append(l.fitness.values[0])\n",
    "    return fitnesses\n",
    "\n",
    "def evolve(evaluator):\n",
    "    \n",
    "    \n",
    "    \n",
    "    creator.create(\"FitnessMax\", base.Fitness, weights=(1.0,) )\n",
    "    creator.create(\"Individual\", list, fitness=creator.FitnessMax)\n",
    "    \n",
    "    toolbox = base.Toolbox()\n",
    "    toolbox.register(\"individual\", tools.initIterate, creator.Individual, genFuncList)\n",
    "    \n",
    "    def myMutate(individual):\n",
    "        new = [[gp.mutNodeReplacement(x,pset=pset) for x in l] for l in toolbox.clone(individual)]\n",
    "        new = [[x[0] for x in l] for l in new]\n",
    "        return (tools.initIterate(creator.Individual, lambda: new),)\n",
    "\n",
    "    toolbox.register(\"mate\", tools.cxUniform, indpb=0.5)\n",
    "    toolbox.register(\"mutate\", myMutate)\n",
    "    toolbox.register(\"select\", tools.selTournament, tournsize = 4)\n",
    "    toolbox.register(\"evaluate\", evaluator)\n",
    "    toolbox.register(\"population\", tools.initRepeat, list, toolbox.individual)\n",
    "    pop = toolbox.population(POPSIZE)\n",
    "    \n",
    "    \n",
    "\n",
    "    myStats = tools.Statistics()\n",
    "    myStats.register(\"mean\", lambda ls: np.mean(getFitMap(ls)))\n",
    "    myStats.register(\"min\", lambda ls: min(getFitMap(ls)))\n",
    "    myStats.register(\"max\", lambda ls: max(getFitMap(ls)))\n",
    "    myStats.register(\"stdDev\", lambda ls: np.std(getFitMap(ls)))\n",
    "\n",
    "    hallOFame = tools.HallOfFame(5) # hall of fame of size 5\n",
    "\n",
    "\n",
    "    (finalPop, logbook) = algorithms.eaMuPlusLambda(pop, toolbox, MU, LAMBDA, CXPB, MUTPB, GENERATIONS, myStats, halloffame=hallOFame, verbose=True)\n",
    "\n",
    "    gen = logbook.select(\"gen\")\n",
    "    fit_maxs = logbook.select(\"max\")\n",
    "    fit_avgs = logbook.select(\"mean\")\n",
    "\n",
    "    import matplotlib.pyplot as plt\n",
    "\n",
    "    fig, ax1 = plt.subplots()\n",
    "    line1 = ax1.plot(gen, fit_maxs, \"b-\", label=\"Maximum Fitness\")\n",
    "    ax1.set_xlabel(\"Generation\")\n",
    "    ax1.set_ylabel(\"Fitness\", color=\"b\")\n",
    "    for tl in ax1.get_yticklabels():\n",
    "        tl.set_color(\"b\")\n",
    "\n",
    "    ax2 = ax1.twinx()\n",
    "    line2 = ax2.plot(gen, fit_avgs, \"r-\", label=\"Average Fitness\")\n",
    "    ax2.set_ylabel(\"Size\", color=\"r\")\n",
    "    for tl in ax2.get_yticklabels():\n",
    "        tl.set_color(\"r\")\n",
    "\n",
    "    lns = line1 + line2\n",
    "    labs = [l.get_label() for l in lns]\n",
    "    ax1.legend(lns, labs, loc=\"center right\")\n",
    "\n",
    "    plt.show()\n",
    "    \n",
    "def flatten(mylist):\n",
    "    return list(itertools.chain.from_iterable(mylist))\n",
    "    \n",
    "def myEval(individual):\n",
    "    funcs = []\n",
    "    for dimlist in individual:\n",
    "        newDimlist = []\n",
    "        for tree in dimlist:\n",
    "#             print gp.stringify(tree)\n",
    "            f = gp.compile(tree, pset)\n",
    "            newDimlist.append(f)\n",
    "        funcs.append(newDimlist)\n",
    "    flattened = flatten(funcs)\n",
    "    mapped = map(lambda f: f(1), flattened)\n",
    "    return max(mapped),\n",
    "    \n",
    "    \n",
    "evolve(myEval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.01171875,  0.0703125 ,  0.0703125 ,\n",
       "        0.0703125 ,  0.4921875 ,  0.53125   ,  0.68359375,  0.1015625 ,\n",
       "        0.6484375 ,  0.99609375,  0.96484375,  0.49609375,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.1171875 ,  0.140625  ,  0.3671875 ,  0.6015625 ,\n",
       "        0.6640625 ,  0.98828125,  0.98828125,  0.98828125,  0.98828125,\n",
       "        0.98828125,  0.87890625,  0.671875  ,  0.98828125,  0.9453125 ,\n",
       "        0.76171875,  0.25      ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.19140625,  0.9296875 ,\n",
       "        0.98828125,  0.98828125,  0.98828125,  0.98828125,  0.98828125,\n",
       "        0.98828125,  0.98828125,  0.98828125,  0.98046875,  0.36328125,\n",
       "        0.3203125 ,  0.3203125 ,  0.21875   ,  0.15234375,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.0703125 ,  0.85546875,  0.98828125,  0.98828125,\n",
       "        0.98828125,  0.98828125,  0.98828125,  0.7734375 ,  0.7109375 ,\n",
       "        0.96484375,  0.94140625,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.3125    ,  0.609375  ,  0.41796875,  0.98828125,  0.98828125,\n",
       "        0.80078125,  0.04296875,  0.        ,  0.16796875,  0.6015625 ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.0546875 ,\n",
       "        0.00390625,  0.6015625 ,  0.98828125,  0.3515625 ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.54296875,\n",
       "        0.98828125,  0.7421875 ,  0.0078125 ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.04296875,  0.7421875 ,  0.98828125,\n",
       "        0.2734375 ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.13671875,  0.94140625,  0.87890625,  0.625     ,\n",
       "        0.421875  ,  0.00390625,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.31640625,  0.9375    ,  0.98828125,  0.98828125,  0.46484375,\n",
       "        0.09765625,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.17578125,\n",
       "        0.7265625 ,  0.98828125,  0.98828125,  0.5859375 ,  0.10546875,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.0625    ,  0.36328125,\n",
       "        0.984375  ,  0.98828125,  0.73046875,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.97265625,  0.98828125,\n",
       "        0.97265625,  0.25      ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.1796875 ,  0.5078125 ,\n",
       "        0.71484375,  0.98828125,  0.98828125,  0.80859375,  0.0078125 ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.15234375,\n",
       "        0.578125  ,  0.89453125,  0.98828125,  0.98828125,  0.98828125,\n",
       "        0.9765625 ,  0.7109375 ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.09375   ,  0.4453125 ,  0.86328125,  0.98828125,  0.98828125,\n",
       "        0.98828125,  0.98828125,  0.78515625,  0.3046875 ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.08984375,  0.2578125 ,  0.83203125,  0.98828125,\n",
       "        0.98828125,  0.98828125,  0.98828125,  0.7734375 ,  0.31640625,\n",
       "        0.0078125 ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.0703125 ,  0.66796875,  0.85546875,\n",
       "        0.98828125,  0.98828125,  0.98828125,  0.98828125,  0.76171875,\n",
       "        0.3125    ,  0.03515625,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.21484375,  0.671875  ,\n",
       "        0.8828125 ,  0.98828125,  0.98828125,  0.98828125,  0.98828125,\n",
       "        0.953125  ,  0.51953125,  0.04296875,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.53125   ,  0.98828125,  0.98828125,  0.98828125,\n",
       "        0.828125  ,  0.52734375,  0.515625  ,  0.0625    ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.        ,  0.        ], dtype=float32)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import mnist\n",
    "import numpy as np\n",
    "import theano\n",
    "training = list(mnist.read(dataset = \"training\"))\n",
    "testing = list(mnist.read(dataset = \"testing\"))\n",
    "\n",
    "def labelToArray(x):\n",
    "    blank = [0] * 10\n",
    "    blank[x] = 1\n",
    "    return np.array(blank)\n",
    "\n",
    "trainingimgs = [train[1].astype(theano.config.floatX).ravel() * (1.0/256) for train in training]\n",
    "traininglabels = [labelToArray(train[0]).astype(theano.config.floatX) for train in training]\n",
    "testingimgs = [test[1].astype(theano.config.floatX).ravel() * (1.0/256) for test in testing]\n",
    "testinglabels = [labelToArray(test[0]).astype(theano.config.floatX) * (1.0 / 256) for test in testing]\n",
    "\n",
    "trainingimgs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running initial singleGenConsolidate\n",
      "starting a singleGenConsolidate\n",
      "half way done a singleGenConsolidate\n",
      "finished a singleGenConsolidate\n",
      "starting to loop\n",
      "starting a singleGenConsolidate\n",
      "half way done a singleGenConsolidate\n",
      "finished a singleGenConsolidate\n",
      "first version took: 0.000173807144165\n",
      "looped\n",
      "starting a singleGenConsolidate\n",
      "half way done a singleGenConsolidate\n",
      "finished a singleGenConsolidate\n",
      "first version took: 0.0001540184021\n",
      "looped\n",
      "starting a singleGenConsolidate\n",
      "half way done a singleGenConsolidate\n",
      "finished a singleGenConsolidate\n",
      "first version took: 0.000154972076416\n",
      "looped\n",
      "starting a singleGenConsolidate\n",
      "half way done a singleGenConsolidate\n",
      "finished a singleGenConsolidate\n",
      "first version took: 0.000158071517944\n",
      "looped\n",
      "starting a singleGenConsolidate\n",
      "half way done a singleGenConsolidate\n",
      "finished a singleGenConsolidate\n",
      "first version took: 0.000166177749634\n",
      "looped\n",
      "starting a singleGenConsolidate\n",
      "half way done a singleGenConsolidate\n",
      "finished a singleGenConsolidate\n",
      "first version took: 0.000154972076416\n",
      "looped\n",
      "starting a singleGenConsolidate\n",
      "half way done a singleGenConsolidate\n",
      "finished a singleGenConsolidate\n",
      "first version took: 0.000152111053467\n",
      "looped\n",
      "starting a singleGenConsolidate\n",
      "half way done a singleGenConsolidate\n",
      "finished a singleGenConsolidate\n",
      "first version took: 0.000153064727783\n",
      "looped\n",
      "[ 0.93820096]\n",
      "biased and tanned: [ 0.04408862  0.07158857  0.06487364  0.18927555  0.07557143]\n",
      "biased and tanned: [ 0.01636772  0.05831546  0.04112239  0.04844521  0.02884416]\n",
      "biased and tanned: [ 0.03316154  0.02267201  0.04299259  0.0303378   0.0176501 ]\n",
      "biased and tanned: [ 0.00249707  0.0390876   0.03305502  0.04861817  0.00910204]\n",
      "biased and tanned: [ 0.00950003  0.02723151  0.02849113  0.00567916  0.01975187]\n",
      "biased and tanned: [ 0.00022523  0.04764779  0.02825208  0.00774352  0.00794655]\n",
      "biased and tanned: [ 0.04521414  0.0299231   0.01892255  0.01104637  0.03145993]\n",
      "biased and tanned: [ 0.02693113  0.02440411  0.03740145  0.04333505  0.01544306]\n",
      "biased and tanned: [ 0.03497066  0.00492714  0.01980029  0.04440942  0.0269134 ]\n",
      "[ 0.93820096]\n"
     ]
    }
   ],
   "source": [
    "import theano.tensor as T\n",
    "from theano import shared\n",
    "import theano\n",
    "from autograd import grad as Grad\n",
    "import autograd.numpy as np\n",
    "import autograd.numpy.linalg as LA\n",
    "import itertools\n",
    "import typing\n",
    "import time\n",
    "import math\n",
    "from itertools import izip\n",
    "SPACESPAN = 2.0\n",
    "\n",
    "def box(x):\n",
    "    if x > 0 + SPACESPAN / 2.0:\n",
    "        return -1.0 + x\n",
    "    elif x < 0 - SPACESPAN / 2.0:\n",
    "        return 1.0 + x\n",
    "    else:\n",
    "        return x\n",
    "            \n",
    "def discretize(x, resDenominator):\n",
    "    if x > 0.0:\n",
    "        return np.floor(1.0 * x * resDenominator) / resDenominator\n",
    "    elif x < 0.0:\n",
    "        return np.floor(1.0 * x * resDenominator) / resDenominator\n",
    "    else:\n",
    "        return 0.0\n",
    "    \n",
    "def mnistlabelToArray(x):\n",
    "    blank = [0] * 10\n",
    "    blank[x] = 1\n",
    "    return np.array(blank)\n",
    "\n",
    "\n",
    "def flatten(mylist):\n",
    "    return list(itertools.chain.from_iterable(mylist))\n",
    "\n",
    "def newSingleGenConsolidate(arrays):\n",
    "    bigMatrix = np.array(arrays)\n",
    "    arraylen = len(arrays)\n",
    "    initArraylen = len(arrays)\n",
    "    indices = []\n",
    "    flattenedIndices = []\n",
    "    \n",
    "    irange = range(arraylen)\n",
    "    \n",
    "    counter = True\n",
    "    print \"starting a singleGenConsolidate\"\n",
    "    for i in irange:\n",
    "        if i not in flattenedIndices:\n",
    "            subgroupArr = np.argwhere(np.all((bigMatrix-bigMatrix[i])==0, axis=0))\n",
    "            subgroup = subgroupArr.tolist()\n",
    "            indices.append(subgroup)\n",
    "            flattenedIndices += subgroup\n",
    "            arraylen -= subgroupArr.size\n",
    "            if counter and arraylen/initArraylen < 0.5:\n",
    "                print \"half way done a singleGenConsolidate\"\n",
    "                counter = False\n",
    "    \n",
    "    print \"finished a singleGenConsolidate\"\n",
    "    return indices\n",
    "            \n",
    "            \n",
    "            \n",
    "def singleGenConsolidate(arrays):\n",
    "    indices = []\n",
    "    flattenedIndices = []\n",
    "    arraylen = len(arrays)\n",
    "    initArraylen = len(arrays) * 1.0\n",
    "    irange = range(arraylen)\n",
    "    \n",
    "    counter = True\n",
    "    print \"starting a singleGenConsolidate\"\n",
    "    for i in irange:\n",
    "        if i not in flattenedIndices:\n",
    "            subgroup = [i] + [j for j in irange[i + 1:] if np.array_equal(arrays[i], arrays[j])]\n",
    "            arraylen -= len(subgroup)\n",
    "            indices.append(subgroup)\n",
    "            flattenedIndices += subgroup\n",
    "            if counter and arraylen/initArraylen < 0.5:\n",
    "                print \"half way done a singleGenConsolidate\"\n",
    "                counter = False\n",
    "    \n",
    "    print \"finished a singleGenConsolidate\"\n",
    "    return indices\n",
    "\n",
    "\n",
    "\n",
    "def nprelu(x):\n",
    "    return x * (x > 0)\n",
    "nprelu = np.vectorize(nprelu)\n",
    "\n",
    "\n",
    "def getSubV2np(index, inds, complete):\n",
    "    return complete[inds[index]:inds[index + 1]]\n",
    "\n",
    "def getSubV3np(index, indinds, inds, complete):\n",
    "    subInds = inds[indinds[index] : indinds[index + 1] + 1]\n",
    "    return subInds, complete\n",
    "    \n",
    "def singleConsol(tensor, indices):\n",
    "    return (np.choose(indices, tensor)).sum()\n",
    "\n",
    "def positionConsolidate(arrays, consolidations):\n",
    "    return [arrays[sublist[0]] for sublist in consolidations]\n",
    "\n",
    "def numpyConsolidate(array, consols):\n",
    "    new = []\n",
    "    for sublist in consols:\n",
    "        x = 0.0\n",
    "        for i in sublist:\n",
    "            x += array[i]\n",
    "        new.append(x)\n",
    "\n",
    "    return new\n",
    "\n",
    "def npConsolidate(array, consolinds, consols):\n",
    "    ordered = array.take(consols)\n",
    "    f = lambda i : ordered[consolinds[i]: consolinds[i + 1]].sum()\n",
    "    veced = np.vectorize(f)\n",
    "    return veced(np.arange(consolinds.size))\n",
    "\n",
    "def generalfeedforward((weights, biases, finalweights, finalbiases), consolMasks, hiddenlayers, branchMultiplier, inp):\n",
    "    \n",
    "    for i in range(hiddenlayers):\n",
    "        inp = np.repeat(inp, branchMultiplier)\n",
    "        inp = np.multiply(weights[i], inp)\n",
    "        inp = np.concatenate((inp, np.array([0.0]).astype('float32')))\n",
    "        inp = inp.take(consolMasks[i]).sum(axis = 1)\n",
    "        inp = nprelu(np.add(inp, biases[i]))\n",
    "    \n",
    "    print inp\n",
    "    finalout = nprelu(np.dot(inp, finalweights) + finalbiases)\n",
    "    return finalout\n",
    "\n",
    "\n",
    "class nnet:\n",
    "    weights = []\n",
    "    biases = []\n",
    "    consolidations = []\n",
    "    finalweights = []\n",
    "    finalbiases = []\n",
    "    weightnum = 0\n",
    "    hiddenlayers = 0\n",
    "    \n",
    "    def __init__(self, resolution, functions, inputdimension, datainp, dataoutp, synapseThreshold):\n",
    "        self.resDenominator = resolution\n",
    "        self.funcs = functions\n",
    "        self.dimensions = len(self.funcs)\n",
    "        self.branchMultiplier = len(flatten(self.funcs))\n",
    "        self.TBranchMult = shared(np.array([self.branchMultiplier]))\n",
    "#         self.dataset = [shared(dat.astype(theano.config.floatX)) for dat in datainp]\n",
    "#         self.datalabels = [shared(outp.astype(theano.config.floatX)) for outp in dataoutp]\n",
    "        self.dataSample = datainp[0] #numpy\n",
    "        self.dataOutSample = dataoutp[0]\n",
    "        self.inputdimension = inputdimension\n",
    "        \n",
    "        if self.dataSample.size ** (1.0/ inputdimension) > self.resDenominator * 2.0:\n",
    "            print \"resDenominator may be too small for effective learning\"\n",
    "            \n",
    "        self.threshold = synapseThreshold\n",
    "\n",
    "    def applyFuncs(self, narray): #checked\n",
    "        boxvec = np.vectorize(lambda x: discretize(box(x), self.resDenominator))\n",
    "        displaced = []\n",
    "        \n",
    "        for dim in range(self.dimensions):\n",
    "            for f in self.funcs[dim]:\n",
    "                \n",
    "                zeroes = np.zeros_like(narray)\n",
    "                displacement = f(narray[dim]) % SPACESPAN\n",
    "                zeroes[dim] = displacement\n",
    "                \n",
    "                newArray = np.add(narray, zeroes)\n",
    "                newArrayBoxed = boxvec(newArray)\n",
    "                displaced.append(newArrayBoxed)\n",
    "                \n",
    "        \n",
    "        return displaced\n",
    "        \n",
    "\n",
    "    def applyFuncsMult(self, narrays): \n",
    "#         print \"narrays: \" + str(narrays)\n",
    "        return flatten([self.applyFuncs(x) for x in narrays])\n",
    "    \n",
    "    def locate(self, sample):  #only 1 or 2 implemented\n",
    "        insize = sample.size\n",
    "        sample = np.ravel(sample) \n",
    "        tensorFrame = [0] * self.dimensions\n",
    "        located = [0] * insize\n",
    "        \n",
    "        boxvec = np.vectorize(lambda x: discretize(box(x), self.resDenominator))\n",
    "        \n",
    "        #self.inputdimension is the dimension the input should be represented in\n",
    "        \n",
    "        if self.inputdimension == 1:\n",
    "            \n",
    "            for i in range(insize):\n",
    "                myTens = tensorFrame[:]\n",
    "                myTens[0] = (2.0 * i) / insize - 1.0\n",
    "                located[i] = boxvec(np.array(myTens))\n",
    "        \n",
    "        if self.inputdimension == 2:\n",
    "            located = []\n",
    "            factorPairs = [(i,(insize / i)) for i in range(1, int(math.floor(insize**0.5))) if insize % i == 0]\n",
    "            pair = factorPairs[-1]\n",
    "            for i in range(pair[0]):\n",
    "                for j in range(pair[1]):\n",
    "                    myTens = tensorFrame[:]\n",
    "                    myTens[0] = 2.0 * i/pair[0] - 1.0\n",
    "                    myTens[1] = 2.0 * j/pair[1] - 1.0\n",
    "                    located.append(boxvec(np.array(myTens)))\n",
    "                            \n",
    "        return located\n",
    "        \n",
    "    def genConsolidate(self): #finds consolidation list from located self.dataSample\n",
    "        consols = []\n",
    "#         print len(self.dataSample.ravel())\n",
    "        located = self.locate(self.dataSample)\n",
    "#         print len(located)\n",
    "#         print type(located[0])\n",
    "        print \"running initial singleGenConsolidate\"\n",
    "        located = self.applyFuncsMult(located)\n",
    "        consols.append(singleGenConsolidate(located))\n",
    "        located = positionConsolidate(located, consols[-1]) \n",
    "\n",
    "#         print located\n",
    "        print \"starting to loop\"\n",
    "        while len(flatten(flatten(consols))) + len(flatten(consols[-1])) * self.branchMultiplier < self.threshold:\n",
    "            located = self.applyFuncsMult(located)\n",
    "            testStart = time.time()\n",
    "            consols.append(singleGenConsolidate(located))\n",
    "            print \"first version took: \" + str(time.time() - testStart)\n",
    "#             testStart = time.time()\n",
    "#             x = newSingleGenConsolidate(located)\n",
    "#             print \"new version took: \" + str(time.time() - testStart)\n",
    "            \n",
    "            located = positionConsolidate(located, consols[-1]) \n",
    "            print \"looped\"\n",
    "            \n",
    "        self.consolidations = consols\n",
    "        self.consolMasks = []\n",
    "        for layerConsols in self.consolidations:\n",
    "#             print \"initial \" + str(layerConsols)\n",
    "            maxInLength = max(map(len, layerConsols))\n",
    "            inLength = len(flatten(layerConsols))\n",
    "            extended = map(lambda l: l + [inLength] * (maxInLength - len(l)), layerConsols)\n",
    "#             print \"extended: \" + str(extended)\n",
    "            self.consolMasks.append(np.array(extended).astype('int32'))\n",
    "        return consols\n",
    "    \n",
    "    def genWeights(self):\n",
    "        myWeightNum = 0\n",
    "        self.hiddenlayers = 0\n",
    "        self.weights = []\n",
    "        self.biases = []\n",
    "        self.finalweights = []\n",
    "        self.finalbiases = []\n",
    "        for consol in self.consolidations:\n",
    "            weightVec = (np.random.rand(len(flatten(consol))) * 0.05).astype(theano.config.floatX).tolist()\n",
    "            biasVec   = (np.random.rand(len(consol)) * 0.05).astype(theano.config.floatX).tolist()\n",
    "            self.weights.append(weightVec)\n",
    "            self.biases.append(biasVec)\n",
    "            myWeightNum += len(weightVec)\n",
    "\n",
    "            #convert to shared data type for later speed\n",
    "            self.hiddenlayers += 1\n",
    "        \n",
    "        outsize = len(self.dataOutSample)\n",
    "        \n",
    "        self.finalweights = np.random.rand(len(self.consolidations[-1]),outsize).astype('float32')        #final interconnected layer for output\n",
    "        self.finalbiases = np.random.rand(outsize).astype('float32')\n",
    "\n",
    "        myWeightNum += len(self.consolidations[-1]) * outsize\n",
    "#         self.weightNum = myWeightNum\n",
    "        \n",
    "    def feedforward(self, inp):\n",
    "        def foo(x):\n",
    "            if x > 0.0:\n",
    "                return x\n",
    "            else:\n",
    "                return 0.0\n",
    "        relu = np.vectorize(foo)\n",
    "        for i in range(self.hiddenlayers):\n",
    "            inp = np.repeat(inp, self.branchMultiplier)\n",
    "#             print \"repeated: \" + str(inp)\n",
    "            inp = np.multiply(self.weights[i], inp)\n",
    "#             print \"weighted: \" + str(inp)\n",
    "            inp = numpyConsolidate(inp, self.consolidations[i])\n",
    "#             print \"consolidated: \" + str(inp)\n",
    "            inp = nprelu(np.add(inp, self.biases[i]))\n",
    "            print \"biased and tanned: \" + str(inp)\n",
    "        \n",
    "#         print \"before finals: \" + str(inp)\n",
    "#         print \"finalweights: \" + str(self.finalweights)\n",
    "#         print \"finalbiases: \" + str(self.finalbiases)\n",
    "        finalout = nprelu(np.dot(inp, self.finalweights) + self.finalbiases)\n",
    "#         print \"final: \" + str(finalout)\n",
    "        return finalout\n",
    "\n",
    "    def nfeedforward(self, inp):\n",
    "        relu = np.vectorize(lambda x: x * (x > 0))\n",
    "        for i in range(self.hiddenlayers):\n",
    "            inp = np.repeat(inp, self.branchMultiplier)\n",
    "            inp = np.multiply(self.weights[i], inp)\n",
    "            inp = np.concatenate((inp, np.array([0.0]).astype('float32')))\n",
    "            inp = inp.take(self.consolMasks[i]).sum(axis = 1)\n",
    "            inp = relu(np.add(inp, self.biases[i]))\n",
    "        return relu(np.dot(inp, self.finalweights) + self.finalbiases)\n",
    "    \n",
    "    def test(inputs, outputs):\n",
    "        def foo(x):\n",
    "            if x > 0.5:\n",
    "                return 1.0\n",
    "            return 0.0\n",
    "\n",
    "        binarize = np.vectorize(foo)\n",
    "        \n",
    "        errors = [LA.norm(outp - binarize(self.feedforward(inp))) for inp,outp in izip(inputs,outputs)]\n",
    "        return sum(errors)/len(inputs) * 100.0\n",
    "        \n",
    "    \n",
    "    def numpytrain(self, alpha, epochs, batchsize, inputlist, outputlist, verbose, testdatainputs, testdatalabels):\n",
    "        print \"starting training\"\n",
    "        \n",
    "        def error((weights, biases, finalweights, finalbiases), inp, outp):\n",
    "            return LA.norm(outp - generalfeedforward((weights,biases,finalweights,finalbiases), \n",
    "                                                     self.consolMasks, self.hiddenlayers, self.branchMultiplier, inp))\n",
    "        error_grad = Grad(error)\n",
    "        \n",
    "        inlen = len(inputlist)\n",
    "        outlen = len(outputlist)\n",
    "\n",
    "        if inlen != outlen:\n",
    "            Exception(\"number of input vectors (\"+str(inlen)+\") not equal not number of ouptut vectors (\"+str(outlen)+\")\")\n",
    "            \n",
    "        if alpha == 0.0:\n",
    "            raise(\"why is alpha zero?\")\n",
    "            \n",
    "        if type(batchsize) != int:\n",
    "            raise(\"Why is batchsize not an int?\")\n",
    "        \n",
    "        if batchsize == 0 or batchsize > inlen:\n",
    "            raise(\"batchsize of \"+str(batchsize)+\"is not allowed. Note than inputveclist has length \"+str(inlen))\n",
    "            \n",
    "        #just some error checks\n",
    "        \n",
    "        randindexlist = range(inlen)\n",
    "        start = time.time()\n",
    "        avgpercenttesterror = \"No test set\"\n",
    "        avgpercenttesterrorlist = []\n",
    "        \n",
    "        #using stochastic gradient descent training method\n",
    "        for epoch in xrange(epochs):\n",
    "            #random.shuffle(randindexlist)\n",
    "            #for batch in range(inlen/batchsize):\n",
    "            \n",
    "#                 low = batch * batchsize\n",
    "#                 upper = (batch + 1) * batchsize\n",
    "#                 if upper > inlen: upper = inlen \n",
    "                \n",
    "#                 total_Egradweights = None\n",
    "#                 total_Egradbiases = None\n",
    "                \n",
    "#                 for i in randindexlist[low:upper]:\n",
    "            for inp,outp in izip(inputlist,outputlist):\n",
    "                Egradws, Egradbs, Egadfws, Egradfbs = error_grad((self.weights, self.biases, \n",
    "                                                       self.finalweights, self.finalbiases), inp, outp) #formerly used indexes from randindexlist\n",
    "#                     if total_Egradweights:\n",
    "#                         total_Egradweights = [x + y for x,y in izip(Egradweights,total_Egradweights)]\n",
    "#                         total_Egradbiases = [x + y for x,y in izip(Egradbiases,total_Egradbiases)]\n",
    "#                     else:\n",
    "#                         total_Egradweights = Egradweights\n",
    "#                         total_Egradbiases = Egradbiases\n",
    "                \n",
    "#                 total_Egradweights = [x / (batchsize * 1.0) for x in total_Egradweights]\n",
    "#                 total_Egradbiases = [x / (batchsize * 1.0) for x in total_Egradbiases]\n",
    "                \n",
    "#                 self.weights = [curr-grad*alpha for curr, grad in izip(self.weights, total_Egradweights)]\n",
    "#                 self.biases = [curr-grad*alpha for curr, grad in izip(self.biases, total_Egradbiases)]\n",
    "                self.weights = [curr-grad*alpha for curr, grad in izip(self.weights, Egradws)]\n",
    "                self.biases = [curr-grad*alpha for curr, grad in izip(self.biases, Egradbs)]\n",
    "                self.finalweights = self.finalweights - alpha * Egradfws\n",
    "                self.finalbiases = self.finalbiases - alpha * Egradfbs\n",
    "            \n",
    "            if len(testinputlist) == len(testoutputlist) and len(testinputlist) != 0:\n",
    "                avgpercenttesterror = self.test(testinputlist, testoutputlist)\n",
    "                avgpercenttesterrorlist.append(avgpercenttesterror)\n",
    "            else:\n",
    "                print \"bad test set\"\n",
    "            \n",
    "            elapsed = time.time() - start\n",
    "            start = time.time()\n",
    "            if verbose:\n",
    "                print \"epoch: \" + str(epoch) + \"\\t percenterror: \" + str(avgpercenttesterror) + \"% \\t time elapsed this epoch: \" + str(elapsed) + \"s\"\n",
    "            \n",
    "        return (avgpercenttesterror, avgpercenttesterrorlist)\n",
    "\n",
    "# singleGenConsolidate([np.array([0,0,0]), np.array([0,1,2,3]), np.array([0,0,0])])\n",
    "testnet = nnet(10, [[lambda x: x + 1], [lambda x: x * 2]], 1, [np.array([0,1,2,3])], [np.array([1])], 100)\n",
    "testnet.genConsolidate()\n",
    "testnet.genWeights()\n",
    "numpyConsolidate(np.array([1,2,3]), [[0,2], [1]])\n",
    "print testnet.nfeedforward(np.array([0,1,2,3]))\n",
    "print testnet.feedforward(np.array([0,1,2,3]))\n",
    "# print mynet.consolidations\n",
    "        \n",
    "        \n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.  1.]\n",
      "running initial singleGenConsolidate\n",
      "starting a singleGenConsolidate\n",
      "half way done a singleGenConsolidate\n",
      "finished a singleGenConsolidate\n",
      "starting to loop\n",
      "starting a singleGenConsolidate\n",
      "half way done a singleGenConsolidate\n",
      "finished a singleGenConsolidate\n",
      "first version took: 0.000130176544189\n",
      "looped\n",
      "starting a singleGenConsolidate\n",
      "half way done a singleGenConsolidate\n",
      "finished a singleGenConsolidate\n",
      "first version took: 0.00011682510376\n",
      "looped\n",
      "starting a singleGenConsolidate\n",
      "half way done a singleGenConsolidate\n",
      "finished a singleGenConsolidate\n",
      "first version took: 0.000127077102661\n",
      "looped\n",
      "starting a singleGenConsolidate\n",
      "half way done a singleGenConsolidate\n",
      "finished a singleGenConsolidate\n",
      "first version took: 0.000118970870972\n",
      "looped\n",
      "starting a singleGenConsolidate\n",
      "half way done a singleGenConsolidate\n",
      "finished a singleGenConsolidate\n",
      "first version took: 0.000164031982422\n",
      "looped\n",
      "starting a singleGenConsolidate\n",
      "half way done a singleGenConsolidate\n",
      "finished a singleGenConsolidate\n",
      "first version took: 0.000115871429443\n",
      "looped\n",
      "starting a singleGenConsolidate\n",
      "half way done a singleGenConsolidate\n",
      "finished a singleGenConsolidate\n",
      "first version took: 0.000116109848022\n",
      "looped\n",
      "starting a singleGenConsolidate\n",
      "half way done a singleGenConsolidate\n",
      "finished a singleGenConsolidate\n",
      "first version took: 0.000116109848022\n",
      "looped\n",
      "biased and tanned: [ 0.0347343   0.06947865  0.08532283  0.14944834  0.13112794]\n",
      "biased and tanned: [ 0.00502553  0.04479414  0.01742804  0.03567469  0.04866783]\n",
      "biased and tanned: [ 0.02624959  0.045532    0.04871766  0.0297341   0.03935651]\n",
      "biased and tanned: [ 0.02320174  0.01215105  0.04823627  0.01270124  0.01075621]\n",
      "biased and tanned: [ 0.03496046  0.00249008  0.04515846  0.00926307  0.01823232]\n",
      "biased and tanned: [ 0.04780548  0.03011359  0.02207352  0.01809545  0.03796417]\n",
      "biased and tanned: [ 0.042976    0.02375921  0.04603051  0.01913313  0.04622111]\n",
      "biased and tanned: [ 0.03190861  0.02779615  0.00999493  0.04863133  0.02476974]\n",
      "biased and tanned: [ 0.0248273   0.01015288  0.0350741   0.01084486  0.03110566]\n",
      "[ 0.49129802  0.18162602  0.07252176  0.69363208]\n",
      "[array([ 0.0347343 ,  0.06947865,  0.08532283,  0.14944834,  0.13112794], dtype=float32), array([ 0.00502553,  0.04479414,  0.01742804,  0.03567469,  0.04866783], dtype=float32), array([ 0.02624959,  0.045532  ,  0.04871766,  0.0297341 ,  0.03935651], dtype=float32), array([ 0.02320174,  0.01215105,  0.04823627,  0.01270124,  0.0107562 ], dtype=float32), array([ 0.03496046,  0.00249008,  0.04515846,  0.00926307,  0.01823232], dtype=float32), array([ 0.04780548,  0.03011359,  0.02207352,  0.01809545,  0.03796417], dtype=float32), array([ 0.042976  ,  0.02375921,  0.04603051,  0.01913313,  0.04622111], dtype=float32), array([ 0.03190861,  0.02779615,  0.00999494,  0.04863133,  0.02476974], dtype=float32), array([ 0.0248273 ,  0.01015288,  0.0350741 ,  0.01084486,  0.03110566], dtype=float32)]\n",
      "CudaNdarray([ -0.00000000e+00  -0.00000000e+00  -7.09794862e-11  -3.09437059e-11\n",
      "  -6.40680911e-11  -1.41958972e-10  -9.61021401e-11  -1.08071579e-10])\n",
      "CudaNdarray([ -2.95477423e-12  -2.95477423e-12  -6.07544431e-11  -6.88297475e-11\n",
      "  -8.45259487e-11  -1.87352998e-11  -1.30682604e-10  -1.30682604e-10\n",
      "  -1.14662634e-10  -9.51916185e-11])\n",
      "CudaNdarray([ -7.73152983e-12  -7.73152983e-12  -6.45888787e-10  -6.45888787e-10\n",
      "  -2.51295790e-10  -1.34689898e-10  -2.75706236e-10  -5.48157020e-10\n",
      "  -7.01743719e-10  -7.19095450e-10])\n",
      "CudaNdarray([ -1.05721532e-09  -1.05721532e-09  -1.30148967e-08  -1.30148967e-08\n",
      "  -1.39254848e-08  -6.58210109e-09  -4.01728784e-09  -7.16572046e-09\n",
      "  -1.12496901e-08  -8.83219364e-09])\n",
      "CudaNdarray([ -1.33953897e-08  -1.33953897e-08  -6.74390037e-08  -6.74390037e-08\n",
      "  -2.67713887e-07  -2.70548526e-07  -7.12389863e-08  -1.29305331e-08\n",
      "  -5.96975198e-08  -1.77088371e-08])\n",
      "CudaNdarray([ -2.90448043e-07  -2.90448043e-07  -1.86532347e-07  -1.86532347e-07\n",
      "  -3.38282462e-06  -3.40887232e-06  -6.99240331e-07  -1.31861659e-08\n",
      "  -1.36578433e-06  -7.22607467e-07])\n",
      "CudaNdarray([ -1.00522138e-05  -1.00522138e-05  -5.97802828e-05  -5.97802828e-05\n",
      "  -4.38194693e-05  -2.90530697e-05  -2.38171451e-05  -6.42616897e-07\n",
      "  -7.53649438e-05  -3.53178402e-05])\n",
      "CudaNdarray([ -9.20780119e-04  -9.20780119e-04  -6.73925213e-04  -6.73925213e-04\n",
      "  -1.30564650e-03  -3.11780081e-04  -1.29595079e-04   2.27289402e-05\n",
      "  -1.31105282e-03  -7.13181274e-04])\n",
      "CudaNdarray([-0.01025385 -0.01025385 -0.00977619 -0.00977619 -0.00351532  0.00172284\n",
      "  0.00838264 -0.02166439 -0.00871177 -0.00523897])\n",
      "[ -0.00000000e+00  -0.00000000e+00  -7.09794862e-11  -3.09437059e-11\n",
      "  -6.40680911e-11  -1.41958972e-10  -9.61021401e-11  -1.08071579e-10]\n",
      "[ -2.95477423e-12  -2.95477423e-12  -6.07544431e-11  -6.88297475e-11\n",
      "  -8.45259487e-11  -1.87352998e-11  -1.30682604e-10  -1.30682604e-10\n",
      "  -1.14662634e-10  -9.51916185e-11]\n",
      "[ -7.73152983e-12  -7.73152983e-12  -6.45888787e-10  -6.45888787e-10\n",
      "  -2.51295790e-10  -1.34689898e-10  -2.75706236e-10  -5.48157020e-10\n",
      "  -7.01743719e-10  -7.19095450e-10]\n",
      "[ -1.05721532e-09  -1.05721532e-09  -1.30148967e-08  -1.30148967e-08\n",
      "  -1.39254848e-08  -6.58210109e-09  -4.01728784e-09  -7.16572046e-09\n",
      "  -1.12496901e-08  -8.83219364e-09]\n",
      "[ -1.33953897e-08  -1.33953897e-08  -6.74390037e-08  -6.74390037e-08\n",
      "  -2.67713887e-07  -2.70548526e-07  -7.12389863e-08  -1.29305331e-08\n",
      "  -5.96975198e-08  -1.77088371e-08]\n",
      "[ -2.90448043e-07  -2.90448043e-07  -1.86532347e-07  -1.86532347e-07\n",
      "  -3.38282462e-06  -3.40887232e-06  -6.99240331e-07  -1.31861659e-08\n",
      "  -1.36578433e-06  -7.22607467e-07]\n",
      "[ -1.00522147e-05  -1.00522147e-05  -5.97802828e-05  -5.97802828e-05\n",
      "  -4.38194693e-05  -2.90530697e-05  -2.38171451e-05  -6.42616897e-07\n",
      "  -7.53649438e-05  -3.53178402e-05]\n",
      "[ -9.20780236e-04  -9.20780236e-04  -6.73925213e-04  -6.73925213e-04\n",
      "  -1.30564650e-03  -3.11780081e-04  -1.29595079e-04   2.27289402e-05\n",
      "  -1.31105282e-03  -7.13181274e-04]\n",
      "[-0.01025385 -0.01025385 -0.00977619 -0.00977619 -0.00351532  0.00172284\n",
      "  0.00838264 -0.02166439 -0.00871177 -0.00523897]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[numpy.ndarray,\n",
       " numpy.ndarray,\n",
       " numpy.ndarray,\n",
       " numpy.ndarray,\n",
       " numpy.ndarray,\n",
       " numpy.ndarray,\n",
       " numpy.ndarray,\n",
       " numpy.ndarray,\n",
       " numpy.ndarray]"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import theano\n",
    "from theano import shared,config,function\n",
    "import itertools\n",
    "import theano.tensor as T\n",
    "import numpy as np\n",
    "from collections import OrderedDict\n",
    "\n",
    "# theano.config.exception_verbosity = \"high\"\n",
    "\n",
    "def flatten(mylist):\n",
    "    return list(itertools.chain.from_iterable(mylist))\n",
    "\n",
    "class V2IndexedShared:\n",
    "    def __init__(self, atype):\n",
    "        self.myType = atype\n",
    "    \n",
    "    def fromList(self,D2List):\n",
    "        self.complete = shared(np.array(flatten(D2List)).astype(self.myType))\n",
    "        self.length = shared(len(D2List))\n",
    "        inds = [0]\n",
    "        for sublist in D2List:\n",
    "            inds.append(len(sublist) + inds[-1])\n",
    "        self.inds = shared(np.array(inds).astype('int32'))\n",
    "        return self\n",
    "\n",
    "    \n",
    "class V3IndexedShared:\n",
    "    def __init__(self,D3List, atype):\n",
    "        self.complete = shared(np.array(flatten(flatten(D3List))).astype(atype))\n",
    "#         self.complete = np.array(flatten(flatten(D3List))).astype(atype)\n",
    "\n",
    "        self.myType = atype\n",
    "        self.length = shared(len(D3List))\n",
    "        D2Inds = [0]\n",
    "        D3Inds = [0]\n",
    "        \n",
    "        for sublistOLists in D3List:\n",
    "            \n",
    "            for sublist in sublistOLists:\n",
    "                D2Inds.append(len(sublist) + D2Inds[-1])\n",
    "                \n",
    "            D3Inds.append(len(sublistOLists) + D3Inds[-1])\n",
    "            \n",
    "                \n",
    "        self.D2Inds = shared(np.array(D2Inds).astype('int32'))\n",
    "        self.D3Inds = shared(np.array(D3Inds).astype('int32'))\n",
    "        \n",
    "#         self.D2Inds = np.array(D2Inds).astype('int32')\n",
    "#         self.D3Inds = np.array(D3Inds).astype('int32')\n",
    "#         print D3Inds\n",
    "#         print D2Inds\n",
    "#         print np.array(flatten(flatten(D3List)))\n",
    "    \n",
    "\n",
    "def relu(x):\n",
    "    return T.maximum(x, 0)\n",
    "\n",
    "\n",
    "def getSubV2(index, inds, complete):\n",
    "    return complete[inds[index]:inds[index + 1]]\n",
    "\n",
    "def getSubV2Check():\n",
    "    tind = T.iscalar('ind')\n",
    "    tinds = T.ivector('inds')\n",
    "    tcomp = T.vector('complete')\n",
    "    \n",
    "    nind = np.asscalar(np.array([0]).astype('int32'))\n",
    "    ninds = np.array([0,2,4]).astype('int32')\n",
    "    comp = np.array([0,1,2,3,4]).astype(theano.config.floatX)\n",
    "    \n",
    "    f = theano.function([tind, tinds, tcomp], getSubV2(tind, tinds, tcomp))\n",
    "    print f(nind,ninds,comp)\n",
    "getSubV2Check()\n",
    "def getV2Length(inds):\n",
    "    return inds.size -1\n",
    "\n",
    "def getSubV3(index, indinds, inds, complete):\n",
    "    subInds = inds[indinds[index] : indinds[index + 1] + 1]\n",
    "    return subInds, complete\n",
    "\n",
    "def getSubV3Check():\n",
    "    D3List = [[[0,1,2,], [3,4,5]], [[6,7,8], [9,10,11]]]\n",
    "    consolcomplete = [0,1,2,3,4,5,6,7,8,9,10,11]\n",
    "    consolinds = [0,3,6,9,12]\n",
    "    consolindinds = [0,2,4]\n",
    "    print getSubV3(1, consolindinds, consolinds, consolcomplete)\n",
    "    print consolinds[2:5]\n",
    "    t = V3IndexedShared(D3List, 'int32')\n",
    "    \n",
    "# getSubV3Check()\n",
    "    \n",
    "def singleConsol(tensor, indices):\n",
    "    return (T.choose(indices, tensor)).sum()\n",
    "\n",
    "def consolidateTensor(tensor, consolinds, consolcomplete):\n",
    "    irange = T.arange(getV2Length(consolinds))\n",
    "    consolidated, updates = theano.scan(fn = lambda ind, tens, coninds, concomp: singleConsol(tens, getSubV2(ind, coninds, concomp)),\n",
    "                                        sequences = irange,\n",
    "                                        non_sequences=[tensor, consolinds, consolcomplete])\n",
    "    return consolidated\n",
    "\n",
    "def consolidateTensorCheck():\n",
    "    x = T.vector('x')\n",
    "    inds = T.ivector('inds')\n",
    "    complete = T.ivector('complete')\n",
    "    y = consolidateTensor(x,inds,complete)\n",
    "    f = theano.function([x,inds,complete], y)\n",
    "    print f(np.array([1.0,2.0,4.0,8.0]).astype(theano.config.floatX), np.array([0,3,4]).astype('int32'), np.array([1,2,3,0]).astype('int32'))\n",
    "\n",
    "\n",
    "    \n",
    "from random import shuffle\n",
    "\n",
    "class tnnet:\n",
    "    def __init__(self, resolution, functions, inputdimension, traindatainps, traindataoutps, testdatainps, testdataoutps, synapseThreshold, net = None):\n",
    "        \n",
    "        mynet = None\n",
    "        if net == None:\n",
    "            mynet = nnet(resolution, functions, inputdimension, traindatainps, traindataoutps, synapseThreshold)\n",
    "            mynet.genConsolidate()\n",
    "            mynet.genWeights()\n",
    "        else:\n",
    "            mynet = net\n",
    "        self.nnet = mynet\n",
    "        \n",
    "\n",
    "        self.sharedTrainingInps = shared(np.array(traindatainps).astype(theano.config.floatX))\n",
    "        self.sharedTrainingOutps = shared(np.array(traindataoutps).astype(theano.config.floatX))\n",
    "        self.sharedTestingInps = shared(np.array(testdatainps).astype(theano.config.floatX))\n",
    "        self.sharedTestingOutps = shared(np.array(testdataoutps).astype(theano.config.floatX))\n",
    "\n",
    "\n",
    "#         print \"weights: \" + str(mynet.weights)\n",
    "#         print \"biases: \" + str(mynet.biases)\n",
    "#         print mynet.feedforward(np.array([0,1,2,3]).astype(theano.config.floatX))\n",
    "#         inp_lengths = [len(consol) for consol in mynet.consolidations]\n",
    "#         inp_lengths = [datainps[0].size] + inp_lengths\n",
    "#         self.inp_lengths = np.array(inp_lengths).astype('int32')\n",
    "#         print \"inp_lengths: \" + str(inp_lengths)\n",
    "#         print \"dinp length: \" + str(datainps[0].size)\n",
    "#         print \"consolidations: \" + str(mynet.consolidations)\n",
    "        \n",
    "#         self.maxInp = np.amax(inp_lengths)\n",
    "#         self.maxPad = shared(np.array([0] * (np.amax(inp_lengths) - min(inp_lengths))).astype(theano.config.floatX))\n",
    "#         self.maxZeroes = shared(np.array([0.0] * self.maxInp).astype(theano.config.floatX))\n",
    "        \n",
    "#         initialPaddingLength = np.amax(inp_lengths) - datainps[0].size\n",
    "#         self.initialPadding = shared(np.array([0.0] * initialPaddingLength).astype(theano.config.floatX))\n",
    "#         self.inp_lengths = shared(self.inp_lengths)\n",
    "        \n",
    "        tweights = V2IndexedShared(theano.config.floatX)\n",
    "        tweights.fromList(mynet.weights)\n",
    "        self.weights = map(lambda x: shared(np.array(x).astype(theano.config.floatX)), mynet.weights)\n",
    "        \n",
    "        tbiases = V2IndexedShared(theano.config.floatX)\n",
    "        tbiases.fromList(mynet.biases)\n",
    "#         print tbiases.complete.get_value()\n",
    "        self.biases = map(lambda x: shared(np.array(x).astype(theano.config.floatX)), mynet.biases)\n",
    "    \n",
    "        tcons = V3IndexedShared(mynet.consolidations, 'int32')\n",
    "#         print \"final weights: \" + str(mynet.finalweights)\n",
    "        tfws = shared((mynet.finalweights).astype(theano.config.floatX))\n",
    "        tfbs = shared((mynet.finalbiases).astype(theano.config.floatX))\n",
    "        self.weightnum = mynet.weightnum\n",
    "        self.hiddenlayers = mynet.hiddenlayers\n",
    "        \n",
    "        self.consolMasks = []\n",
    "        self.dEdWsinverseConsols = []\n",
    "        self.oneMultipliers = []\n",
    "        for layerConsols in mynet.consolidations:\n",
    "#             print \"initial \" + str(layerConsols)\n",
    "            maxInLength = max(map(len, layerConsols))\n",
    "            inLength = len(flatten(layerConsols))\n",
    "            extended = map(lambda l: l + [inLength] * (maxInLength - len(l)), layerConsols)\n",
    "#             print \"extended: \" + str(extended)\n",
    "            self.consolMasks.append(shared(np.array(extended).astype('int32')))\n",
    "            self.oneMultipliers.append(shared(np.array([1.0] * maxInLength).astype(theano.config.floatX)))\n",
    "            \n",
    "            flattened = flatten(layerConsols)\n",
    "            frame = [0] * len(flattened)\n",
    "            for i in range(len(layerConsols)):\n",
    "                for sub in layerConsols[i]:\n",
    "                    frame[sub] = i\n",
    "            \n",
    "            self.dEdWsinverseConsols.append(shared(np.array(frame).astype('int32')))\n",
    "        \n",
    "#         print self.nnet.consolidations[0]\n",
    "#         print self.dEdWsinverseConsols[0].get_value()\n",
    "            \n",
    "        \n",
    "        \n",
    "        inp = T.fvector('inp')\n",
    "#         paddedInp = T.concatenate([inp, self.initialPadding])\n",
    "        actualOutp = T.fvector('actualOutp')\n",
    "        alpha = T.fscalar('alpha')\n",
    "        \n",
    "        self.consolidations = map(lambda x: V2IndexedShared('int32').fromList(x), self.nnet.consolidations)\n",
    "        \n",
    "        self.winds = tweights.inds\n",
    "        self.ws = tweights.complete\n",
    "        self.binds = tbiases.inds\n",
    "        self.bs = tbiases.complete\n",
    "        self.consindinds = tcons.D3Inds\n",
    "        self.consinds = tcons.D2Inds\n",
    "        self.cons = tcons.complete\n",
    "        self.bMult = shared(np.asscalar(np.array([mynet.branchMultiplier]).astype('int32')))\n",
    "        self.fws = tfws\n",
    "        self.fbs = tfbs\n",
    "#         print \"weights: \" + '\\n'.join(map(str, self.nnet.weights))\n",
    "# #         print self.nnet.biases\n",
    "#         print \"inverse Consols: \" + '\\n'.join(map(lambda x: str(x.get_value().tolist()), self.dEdWsinverseConsols))\n",
    "#         print \"consols: \" + '\\n'.join(map(str, self.nnet.consolidations))\n",
    "        self.zeropad = shared(np.array([0.0]).astype(theano.config.floatX))\n",
    "        \n",
    "        self.bMultOnes = shared(np.array([1.0] * self.nnet.branchMultiplier).astype(theano.config.floatX))\n",
    "        \n",
    "        self.params = self.weights + self.biases + [self.fws, self.fbs]\n",
    "#         self.constparams = [self.consindinds, self.consinds, self.cons, self.winds, self.binds, self.bMult]\n",
    "        \n",
    "#         def singleCons(consInds, tens):\n",
    "#             asum, _ = theano.scan(fn = lambda i, tensor: tensor[i],\n",
    "#                                  sequences = consInds,\n",
    "#                                  non_sequences = tens)\n",
    "#             return asum.sum()\n",
    "        \n",
    "        \n",
    "#         def layer(ind, inpLength, inp):\n",
    "#             weights = getSubV2(ind, self.winds, self.ws)\n",
    "#             biases = getSubV2(ind, self.binds, self.bs)\n",
    "#             consolinds, consols = getSubV3(ind, self.consindinds, self.consinds, self.cons)\n",
    "#             inp = T.repeat(inp[0:inpLength], self.bMult)\n",
    "#             inp = weights * inp\n",
    "            \n",
    "#             #consolidation code\n",
    "#             numConsolidatedRange = T.arange(consolinds.size - 1)\n",
    "#             inp, _ = theano.scan(fn = lambda ind, x, coninds, concomp: singleCons(getSubV2(ind, coninds, concomp), x),\n",
    "#                                                 sequences = numConsolidatedRange,\n",
    "#                                                 non_sequences=[inp, consolinds, consols])\n",
    "#             inp = inp + biases\n",
    "#             inp = relu(inp)\n",
    "#             padding = self.maxPad[0 : self.maxInp - inp.size]\n",
    "#             inp = T.concatenate([inp, padding])\n",
    "#             return inp\n",
    "\n",
    "\n",
    "        def gradLayer(ind, nextdEdInputs, layerOutput): #returns (dEdWs, dEdBs, newdEdInputs)\n",
    "            expandedLayerOutput = T.repeat(layerOutput, self.bMult)\n",
    "            inverse = self.dEdWsinverseConsols[ind]\n",
    "            inverseddEdInputs = nextdEdInputs.take(inverse)\n",
    "            dEdWs = expandedLayerOutput * inverseddEdInputs\n",
    "            dEdBs = nextdEdInputs\n",
    "            #upt to here correct\n",
    "            \n",
    "            dOutsdIns = T.gt(layerOutput, T.zeros_like(layerOutput))\n",
    "            dOutsdIns = T.cast(dOutsdIns, 'float32')\n",
    "            \n",
    "            dEdConnections = self.weights[ind] * inverseddEdInputs\n",
    "            dEdConnectionGroups = dEdConnections.reshape((dEdConnections.size // self.bMult, self.bMult))\n",
    "            dEdOuts = T.dot(dEdConnectionGroups, self.bMultOnes) #could need to be axis 1\n",
    "            \n",
    "            newdEdInputs = dEdOuts * dOutsdIns\n",
    "            \n",
    "            return (dEdWs, dEdBs, newdEdInputs)\n",
    "        \n",
    "        def intermediateLoop(z):\n",
    "            ret = []\n",
    "            for i in range(self.hiddenlayers):\n",
    "                z = layer(i, z)\n",
    "                ret.append(z.copy())\n",
    "            \n",
    "            return ret\n",
    "        \n",
    "        def myGrad(theta, actualOutp):\n",
    "            outputs = intermediateLoop(theta)\n",
    "            outputs = [theta] + outputs\n",
    "            last = T.dot(outputs[-1], self.fws)\n",
    "            last = last + self.fbs\n",
    "            last = theano.tensor.nnet.relu(last)\n",
    "            \n",
    "            \n",
    "            \n",
    "            finaldOutdIn = T.gt(last, T.zeros_like(last))\n",
    "            finaldOutdIn = T.cast(finaldOutdIn, 'float32')\n",
    "            \n",
    "            difference = last - actualOutp\n",
    "            err = T.dot(difference, difference)\n",
    "            \n",
    "            initdEdIn = finaldOutdIn * (last - actualOutp)\n",
    "            dEdFbs = initdEdIn   \n",
    "            \n",
    "            \n",
    "            initdEdInshuffled = initdEdIn.dimshuffle(('x',0))\n",
    "            finalStructuredLayerOut = outputs[-1]\n",
    "            finalStructuredLayerOutShuffled = finalStructuredLayerOut.dimshuffle((0,'x'))\n",
    "            \n",
    "            dEdFws = finalStructuredLayerOutShuffled * initdEdInshuffled\n",
    "            \n",
    "            \n",
    "            finalStructuredLayerdOutdIn = T.gt(finalStructuredLayerOut, T.zeros_like(finalStructuredLayerOut))\n",
    "            finalStructuredLayerdOutdIn = T.cast(finalStructuredLayerdOutdIn, 'float32')\n",
    "            \n",
    "            finalStructuredLayerdEdOut = T.dot(initdEdIn, self.fws.T)\n",
    "            \n",
    "            finalStructuredLayerdEdIn = finalStructuredLayerdOutdIn * finalStructuredLayerdEdOut\n",
    "            #up to here is correct\n",
    "\n",
    "            dEdWs, dEdBs = gradLoop(finalStructuredLayerdEdIn, outputs)\n",
    "            \n",
    "            return err, list(reversed(dEdWs)), list(reversed(dEdBs)), dEdFws, dEdFbs #, list(reversed(dEdIns)),\n",
    "            \n",
    "        def gradLoop(dEdIn, layerOutputs):\n",
    "            dEdWs = []\n",
    "            dEdBs = []\n",
    "            for k in reversed(range(self.hiddenlayers)):\n",
    "                newdEdWs, newdEdBs, dEdIn = gradLayer(k, dEdIn, layerOutputs[k])\n",
    "                dEdWs.append(newdEdWs)\n",
    "                dEdBs.append(newdEdBs)\n",
    "            \n",
    "            return dEdWs, dEdBs\n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "\n",
    "        def layer(ind, x):\n",
    "#             print \"in layer\"\n",
    "#             consolinds = self.consolidations[ind].inds\n",
    "#             consols = self.consolidations[ind].complete\n",
    "            \n",
    "            x = T.repeat(x, self.bMult) * self.weights[ind]\n",
    "            x = T.concatenate([x, T.zeros_like(x)])\n",
    "            x = (x.take(self.consolMasks[ind]).astype(theano.config.floatX)) #.sum(axis = 1)\n",
    "            x = T.dot(x, self.oneMultipliers[ind])\n",
    "# #             inp, _ = theano.map(fn = lambda i: inp[i],\n",
    "# #                                sequences = consols)\n",
    "# #             #consolidation code\n",
    "#             r = T.arange(consolinds.size - 1).astype(theano.config.floatX)\n",
    "#             for i in xrange(len(self.nnet.consolidations[ind])):\n",
    "#                 r = T.set_subtensor(r[i], x[consolinds[i]:consolinds[i + 1]].sum())\n",
    "#             numConsolidatedRange = T.arange(consolinds.size - 1).astype('int32')\n",
    "# #             inp2, _ = theano.map(fn = lambda i: inp1[consolinds[i]: consolinds[i + 1]].sum(),\n",
    "# #                                                 sequences = numConsolidatedRange)\n",
    "#             r, _ = theano.map(fn = lambda i, x: x[consolinds[i]: consolinds[i+1]].sum(),\n",
    "#                                                 sequences = numConsolidatedRange,\n",
    "#                                                 non_sequences=[x])\n",
    "            return theano.tensor.nnet.relu(x + self.biases[ind])\n",
    "#             return r\n",
    "            \n",
    "#        = layer(0, inp)\n",
    "        final = T.fvector()\n",
    "        def loop(z):\n",
    "            ret = T.fvector()\n",
    "            for i in range(self.hiddenlayers):\n",
    "                z = layer(i, z)\n",
    "                ret = z\n",
    "            return ret\n",
    "        finals = loop(inp)\n",
    "        final = T.dot(finals, self.fws)\n",
    "        final = final + self.fbs\n",
    "        final = theano.tensor.nnet.relu(final)\n",
    "        \n",
    "        diff = final - actualOutp\n",
    "        squared = 0.5 * T.dot(diff, diff)\n",
    "        self.classify = theano.function([inp], final)\n",
    "#         print self.classify(np.array([0,1,2,3]).astype(theano.config.floatX))\n",
    "\n",
    "        index = T.iscalar()\n",
    "\n",
    "        squaredError, wgrads, bgrads, fwsgrads, fbsgrads = myGrad(inp, actualOutp)\n",
    "#         mygradients = myGrad(inp, actualOutp)\n",
    "        self.realGrad = theano.function(inputs = [inp, actualOutp], outputs = T.grad(squared, self.weights))\n",
    "        self.mygrads = theano.function(inputs = [inp, actualOutp], outputs = wgrads)\n",
    "        gradients = wgrads + bgrads + [fwsgrads, fbsgrads]\n",
    "#         realgrads = T.grad(squared, self.params)\n",
    "#         realUpdates = OrderedDict((p, p - alpha * g) for p, g in zip(self.params, realgrads))\n",
    "        param_Updates = OrderedDict((p, p - alpha * g) for p, g in zip(self.params, gradients))\n",
    "        \n",
    "        self.intermediates = theano.function([inp], intermediateLoop(inp))\n",
    "        \n",
    "        self.train = theano.function(inputs = [index, alpha],\n",
    "                                                                    outputs = squaredError,\n",
    "                                                                    updates = param_Updates,\n",
    "                                                                    givens = {\n",
    "                                                                        inp : self.sharedTrainingInps[index],\n",
    "                                                                        actualOutp: self.sharedTrainingOutps[index]\n",
    "                                                                    })\n",
    "#         self.realtrain = theano.function(inputs = [index, alpha],\n",
    "#                                                                     outputs = squared,\n",
    "#                                                                     updates = realUpdates,\n",
    "#                                                                     givens = {\n",
    "#                                                                         inp : self.sharedTrainingInps[index],\n",
    "#                                                                         actualOutp: self.sharedTrainingOutps[index]\n",
    "#                                                                     })\n",
    "        self.test = theano.function(inputs = [index],\n",
    "            outputs = final,\n",
    "            givens = {\n",
    "                inp : self.sharedTestingInps[index]\n",
    "            })\n",
    "    \n",
    "    def descend(self, alpha, epochs, trainingInps, trainingOutps, testingInps, testingOutps, verbose):\n",
    "        training = zip(trainingInps, trainingOutps)\n",
    "        testing = zip(testingInps, testingOutps)\n",
    "        testLen = len(testing)\n",
    "        trainLen = len(trainingInps)\n",
    "        indorder = range(trainLen)\n",
    "        testingAccuracies = []\n",
    "        \n",
    "        for i in range(epochs):\n",
    "            print \"starting epoch...\"\n",
    "            start = time.time()\n",
    "            shuffle(indorder)\n",
    "            for j in indorder:\n",
    "                self.train(j, alpha)\n",
    "            \n",
    "            testingAccuracy = 0.0\n",
    "            for j in range(len(testingOutps)):\n",
    "                if np.argmax(self.test(j)) == np.argmax(testingOutps[j]):\n",
    "                    testingAccuracy += 1.0\n",
    "            # for (testInp, testOutp) in testing:\n",
    "            #     if np.argmax(self.classify(testInp)) == np.argmax(testOutp):\n",
    "            #         testingAccuracy += 1.0\n",
    "                    \n",
    "            percentTestingAccuracy = testingAccuracy / testLen * 100.0\n",
    "            testingAccuracies.append(percentTestingAccuracy)\n",
    "            end = time.time()\n",
    "            if verbose:\n",
    "                print \"epoch \" + str(i + 1) + \" -- testing accuracy : \" + str(percentTestingAccuracy) + \" duration: \" + str(end - start) + \"s\"\n",
    "        \n",
    "        return max(testingAccuracies), testingAccuracies\n",
    "            \n",
    "        \n",
    "tn = tnnet(10, [[lambda x: x + 1], [lambda x: x * 2]], 1, [np.array([0,1,2,3])], [np.array([1,2,3,4])],  [np.array([1,2,3,4])],  [np.array([1,2,3,4])], 100)\n",
    "print tn.nnet.feedforward(np.array([0,1,2,3]))\n",
    "print tn.intermediates(np.array([0,1,2,3]).astype(theano.config.floatX))\n",
    "# print tn.nnet.weights\n",
    "# print tn.nnet.biases\n",
    "x = np.array([0,1,2,3]).astype(theano.config.floatX)\n",
    "classed = tn.classify(np.array([0,1,2,3]).astype(theano.config.floatX))\n",
    "# print '\\n'.join(map(str,classed))\n",
    "# diff = x * 0.5 - classed\n",
    "# print np.dot(diff, diff)\n",
    "# print tn.classify(np.array([0,1,2,3]).astype(theano.config.floatX))\n",
    "print '\\n'.join(map(str, tn.realGrad(x, x * 0.33)))\n",
    "print '\\n'.join(map(str, tn.mygrads(x, x*0.33)))\n",
    "map(type, tn.mygrads(x, x * 0.33))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evaluating\n",
      "running initial singleGenConsolidate\n",
      "starting a singleGenConsolidate\n",
      "half way done a singleGenConsolidate\n",
      "finished a singleGenConsolidate\n",
      "starting to loop\n",
      "branchmultiplier: 33\n",
      "weights length: 25872\n",
      "average number of connections per neuron by layer: [1.0]\n",
      "spacial dimension: 7\n",
      "branching by layer: [3, 10, 4, 4, 2, 5, 5]\n",
      "beginning training\n",
      "trained in: 0.00302386283875\n",
      "115600.453125\n",
      "[ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      "autograd trained in: 0.00561690330505\n",
      "starting epoch...\n",
      "epoch 1 -- testing accuracy : 9.8 duration: 161.044973135s\n",
      "starting epoch...\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-66-eb3ea0c518e8>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     61\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     62\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 63\u001b[1;33m \u001b[0mevolve\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmnistevaluate\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-2-fe38bb5d846a>\u001b[0m in \u001b[0;36mevolve\u001b[1;34m(evaluator)\u001b[0m\n\u001b[0;32m     67\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     68\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 69\u001b[1;33m     \u001b[1;33m(\u001b[0m\u001b[0mfinalPop\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlogbook\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0malgorithms\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0meaMuPlusLambda\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpop\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtoolbox\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mMU\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mLAMBDA\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mCXPB\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mMUTPB\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mGENERATIONS\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmyStats\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhalloffame\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mhallOFame\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     70\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     71\u001b[0m     \u001b[0mgen\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlogbook\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"gen\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/lib/python2.7/site-packages/deap/algorithms.pyc\u001b[0m in \u001b[0;36meaMuPlusLambda\u001b[1;34m(population, toolbox, mu, lambda_, cxpb, mutpb, ngen, stats, halloffame, verbose)\u001b[0m\n\u001b[0;32m    300\u001b[0m     \u001b[1;31m# Evaluate the individuals with an invalid fitness\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    301\u001b[0m     \u001b[0minvalid_ind\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mind\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mind\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mpopulation\u001b[0m \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mind\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfitness\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalid\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 302\u001b[1;33m     \u001b[0mfitnesses\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtoolbox\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtoolbox\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minvalid_ind\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    303\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mind\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfit\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minvalid_ind\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfitnesses\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    304\u001b[0m         \u001b[0mind\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfitness\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfit\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-66-eb3ea0c518e8>\u001b[0m in \u001b[0;36mmnistevaluate\u001b[1;34m(individual)\u001b[0m\n\u001b[0;32m     57\u001b[0m     \u001b[1;32mprint\u001b[0m \u001b[1;34m\"autograd trained in: \"\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mtestStart\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     58\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 59\u001b[1;33m     \u001b[0mavgpercenttestAccuracy\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mavgpercenttestAccuracylist\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtnet\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdescend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0.02\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m5\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrainingimgs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtraininglabels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtestingimgs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtestinglabels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     60\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mavgpercenttestAccuracy\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     61\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-64-0ad62b9e291b>\u001b[0m in \u001b[0;36mdescend\u001b[1;34m(self, alpha, epochs, trainingInps, trainingOutps, testingInps, testingOutps, verbose)\u001b[0m\n\u001b[0;32m    411\u001b[0m             \u001b[0mshuffle\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindorder\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    412\u001b[0m             \u001b[1;32mfor\u001b[0m \u001b[0mj\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mindorder\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 413\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    414\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    415\u001b[0m             \u001b[0mtestingAccuracy\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0.0\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/lib/python2.7/site-packages/theano/compile/function_module.pyc\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    857\u001b[0m         \u001b[0mt0_fn\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    858\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 859\u001b[1;33m             \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    860\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    861\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'position_of_error'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import deap.gp as gp\n",
    "import time\n",
    "import sys\n",
    "sys.setrecursionlimit(1500)\n",
    "weights = []\n",
    "biases = []\n",
    "branching = 0\n",
    "consolidations = []\n",
    "fws = []\n",
    "fbs = []\n",
    "hiddenlayers = 0\n",
    "weightnum = 0\n",
    "branching = 0\n",
    "def mnistevaluate(individual):\n",
    "    print \"evaluating\"\n",
    "    funcs = []\n",
    "    for dimlist in individual:\n",
    "        newDimlist = []\n",
    "        for tree in dimlist:\n",
    "#             print gp.stringify(tree)\n",
    "            f = gp.compile(tree, pset)\n",
    "            newDimlist.append(f)\n",
    "        funcs.append(newDimlist)\n",
    "    \n",
    "    res = 20\n",
    "    tnet = tnnet(res, funcs, 2, trainingimgs, traininglabels, testingimgs, testinglabels, 200000)\n",
    "    mynet = tnet.nnet\n",
    "    weights = mynet.weights\n",
    "    biases = mynet.biases\n",
    "    branching = mynet.branchMultiplier\n",
    "    consolidations = mynet.consolidations\n",
    "    fws = mynet.finalweights\n",
    "    fbs = mynet.finalbiases\n",
    "    hiddenlayers = mynet.hiddenlayers\n",
    "    weightnum = mynet.weightnum\n",
    "    print \"branchmultiplier: \" + str(mynet.branchMultiplier)\n",
    "    print \"weights length: \" + str(len(flatten(mynet.weights)))\n",
    "#     print \"consolidations lengths\" + str(map(len, mynet.consolidations))\n",
    "    layerDensities = []\n",
    "    prevlayerLength = trainingimgs[0].size\n",
    "    for layer in mynet.weights:\n",
    "        layerLength = len(layer)\n",
    "        branched = prevlayerLength * mynet.branchMultiplier\n",
    "        layerDensities.append((1.0 * branched) / layerLength)\n",
    "        prevlayerLength = layerLength\n",
    "    print \"average number of connections per neuron by layer: \" + str(layerDensities)\n",
    "    print \"spacial dimension: \" + str(len(funcs))\n",
    "    print \"branching by layer: \" + str(map(len, funcs))\n",
    "    print \"beginning training\"\n",
    "    testStart = time.time()\n",
    "    x = tnet.train(0, 0.05)\n",
    "    print \"trained in: \" + str(time.time() - testStart)\n",
    "    print x\n",
    "    print tnet.classify(trainingimgs[0])\n",
    "#     testStart = time.time()\n",
    "#     x = tnet.realtrain(0, 0.05)\n",
    "    print \"autograd trained in: \" + str(time.time() - testStart)\n",
    "    \n",
    "    avgpercenttestAccuracy, avgpercenttestAccuracylist = tnet.descend(0.02, 5, trainingimgs,traininglabels, testingimgs, testinglabels, True)\n",
    "    return avgpercenttestAccuracy\n",
    "\n",
    "\n",
    "evolve(mnistevaluate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(fbs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting theano net instance...\n"
     ]
    },
    {
     "ename": "UnusedInputError",
     "evalue": "theano.function was asked to create a function computing outputs given certain inputs, but the provided input variable at index 0 is not part of the computational graph needed to compute the outputs: inp.\nTo make this error into a warning, you can pass the parameter on_unused_input='warn' to theano.function. To disable it completely, use on_unused_input='ignore'.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mUnusedInputError\u001b[0m                          Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-18-868a88d1e99c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     18\u001b[0m     \u001b[1;32mprint\u001b[0m \u001b[1;34m\"numpy classified in \"\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mtestStart\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 20\u001b[1;33m \u001b[0mtnetFromParams\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mweights\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbiases\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfws\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfbs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconsolidations\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhiddenlayers\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweightnum\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbranching\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-18-868a88d1e99c>\u001b[0m in \u001b[0;36mtnetFromParams\u001b[1;34m(weights, biases, fws, fbs, consolidations, hiddenlayers, weightnum, branching)\u001b[0m\n\u001b[0;32m     10\u001b[0m     \u001b[0mframe\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbranchingmultiplier\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbranching\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m     \u001b[1;32mprint\u001b[0m \u001b[1;34m\"starting theano net instance...\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 12\u001b[1;33m     \u001b[0mmytnet\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtnnet\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mx\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mx\u001b[0m \u001b[1;33m*\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrainingimgs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtraininglabels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtestingimgs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtestinglabels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m5000\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnet\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mframe\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     13\u001b[0m     \u001b[0mtestStart\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m     \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmytnet\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclassify\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrainingimgs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-15-a7fa26851401>\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, resolution, functions, inputdimension, traindatainps, traindataoutps, testdatainps, testdataoutps, synapseThreshold, net)\u001b[0m\n\u001b[0;32m    361\u001b[0m         \u001b[0mfinal\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtheano\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnnet\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfinal\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    362\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 363\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclassify\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtheano\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfunction\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0minp\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfinal\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    364\u001b[0m \u001b[1;31m#         print self.classify(np.array([0,1,2,3]).astype(theano.config.floatX))\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    365\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/lib/python2.7/site-packages/theano/compile/function.pyc\u001b[0m in \u001b[0;36mfunction\u001b[1;34m(inputs, outputs, mode, updates, givens, no_default_updates, accept_inplace, name, rebuild_strict, allow_input_downcast, profile, on_unused_input)\u001b[0m\n\u001b[0;32m    318\u001b[0m                    \u001b[0mon_unused_input\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mon_unused_input\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    319\u001b[0m                    \u001b[0mprofile\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mprofile\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 320\u001b[1;33m                    output_keys=output_keys)\n\u001b[0m\u001b[0;32m    321\u001b[0m     \u001b[1;31m# We need to add the flag check_aliased inputs if we have any mutable or\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    322\u001b[0m     \u001b[1;31m# borrowed used defined inputs\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/lib/python2.7/site-packages/theano/compile/pfunc.pyc\u001b[0m in \u001b[0;36mpfunc\u001b[1;34m(params, outputs, mode, updates, givens, no_default_updates, accept_inplace, name, rebuild_strict, allow_input_downcast, profile, on_unused_input, output_keys)\u001b[0m\n\u001b[0;32m    477\u001b[0m                          \u001b[0maccept_inplace\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0maccept_inplace\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    478\u001b[0m                          \u001b[0mprofile\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mprofile\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mon_unused_input\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mon_unused_input\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 479\u001b[1;33m                          output_keys=output_keys)\n\u001b[0m\u001b[0;32m    480\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    481\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/lib/python2.7/site-packages/theano/compile/function_module.pyc\u001b[0m in \u001b[0;36morig_function\u001b[1;34m(inputs, outputs, mode, accept_inplace, name, profile, on_unused_input, output_keys)\u001b[0m\n\u001b[0;32m   1774\u001b[0m                    \u001b[0mprofile\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mprofile\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1775\u001b[0m                    \u001b[0mon_unused_input\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mon_unused_input\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1776\u001b[1;33m                    \u001b[0moutput_keys\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0moutput_keys\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcreate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1777\u001b[0m             defaults)\n\u001b[0;32m   1778\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/lib/python2.7/site-packages/theano/compile/function_module.pyc\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, inputs, outputs, mode, accept_inplace, function_builder, profile, on_unused_input, fgraph, output_keys)\u001b[0m\n\u001b[0;32m   1413\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1414\u001b[0m         \u001b[1;31m# Check if some input variables are unused\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1415\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_check_unused_inputs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mon_unused_input\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1416\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1417\u001b[0m         \u001b[1;31m# Make a list of (SymbolicInput|SymblicInputKits, indices,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/lib/python2.7/site-packages/theano/compile/function_module.pyc\u001b[0m in \u001b[0;36m_check_unused_inputs\u001b[1;34m(self, inputs, outputs, on_unused_input)\u001b[0m\n\u001b[0;32m   1551\u001b[0m                 \u001b[1;32melif\u001b[0m \u001b[0mon_unused_input\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'raise'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1552\u001b[0m                     raise UnusedInputError(msg % (inputs.index(i),\n\u001b[1;32m-> 1553\u001b[1;33m                                                   i.variable, err_msg))\n\u001b[0m\u001b[0;32m   1554\u001b[0m                 \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1555\u001b[0m                     raise ValueError(\"Invalid value for keyword \"\n",
      "\u001b[1;31mUnusedInputError\u001b[0m: theano.function was asked to create a function computing outputs given certain inputs, but the provided input variable at index 0 is not part of the computational graph needed to compute the outputs: inp.\nTo make this error into a warning, you can pass the parameter on_unused_input='warn' to theano.function. To disable it completely, use on_unused_input='ignore'."
     ]
    }
   ],
   "source": [
    "def tnetFromParams(weights, biases, fws, fbs, consolidations, hiddenlayers, weightnum, branching):\n",
    "    frame = nnet(10, [[lambda x: x + 1], [lambda x: x * 2]], 1, [np.array([0,1,2,3])], [np.array([1])], 100)\n",
    "    frame.weights = weights\n",
    "    frame.biases = biases\n",
    "    frame.finalweights = np.array(fws).astype(theano.config.floatX)\n",
    "    frame.finalbiases = np.array(fbs).astype(theano.config.floatX)\n",
    "    frame.consolidations = consolidations\n",
    "    frame.hiddenlayers = hiddenlayers\n",
    "    frame.weightnum = weightnum\n",
    "    frame.branchingmultiplier = branching\n",
    "    print \"starting theano net instance...\"\n",
    "    mytnet = tnnet(10, [[lambda x: x + 1], [lambda x: x * 2]], 1, trainingimgs, traininglabels, testingimgs, testinglabels, 5000, net = frame)\n",
    "    testStart = time.time()\n",
    "    x = mytnet.classify(trainingimgs[0])\n",
    "    print \"classified in \" + str(time.time() - testStart)\n",
    "    testStart = time.time()\n",
    "    x = frame.feedforward(trainingimgs[0])\n",
    "    print \"numpy classified in \" + str(time.time() - testStart)\n",
    "\n",
    "tnetFromParams(weights, biases, fws, fbs, consolidations, hiddenlayers, weightnum, branching)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost: 0.752302922028\n",
      "Cost: 0.234828531853\n",
      "Cost: 0.217997165391\n",
      "Cost: 0.176582778796\n",
      "Cost: 0.0941889559431\n",
      "Cost: 0.0547008715418\n",
      "Cost: 0.0227962112237\n",
      "Cost: 0.0103721161976\n",
      "Cost: 0.00631427914256\n",
      "Cost: 0.00444667437915\n",
      "Cost: 0.00339910571615\n",
      "Cost: 0.00273639533448\n",
      "Cost: 0.00228238999109\n",
      "Cost: 0.00195317405859\n",
      "Cost: 0.00170428799655\n",
      "Cost: 0.00150985199376\n",
      "Cost: 0.00135402741352\n",
      "Cost: 0.00122653983734\n",
      "Cost: 0.00112027524709\n",
      "Cost: 0.00103046853549\n",
      "0.971243725343\n",
      "0.0324757818475\n",
      "0.97118115104\n",
      "0.0308740290756\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using gpu device 0: GeForce GTX 970 (CNMeM is disabled, cuDNN not available)\n"
     ]
    }
   ],
   "source": [
    "import theano.tensor.nnet as nnet\n",
    "import theano.tensor as T\n",
    "import theano\n",
    "import numpy as np\n",
    "x = T.dvector()\n",
    "y = T.dscalar()\n",
    "def layer(x, w):\n",
    "    b = np.array([1], dtype=theano.config.floatX)\n",
    "    new_x = T.concatenate([x, b])\n",
    "    m = T.dot(w.T, new_x) #theta1: 3x3 * x: 3x1 = 3x1 ;;; theta2: 1x4 * 4x1\n",
    "    h = nnet.sigmoid(m)\n",
    "    return h\n",
    "def grad_desc(cost, theta):\n",
    "    alpha = 0.1 #learning rate\n",
    "    return theta - (alpha * T.grad(cost, wrt=theta))\n",
    "theta1 = theano.shared(np.array(np.random.rand(3,3), dtype=theano.config.floatX)) # randomly initialize\n",
    "theta2 = theano.shared(np.array(np.random.rand(4,1), dtype=theano.config.floatX))\n",
    "hid1 = layer(x, theta1) #hidden layer\n",
    "out1 = T.sum(layer(hid1, theta2)) #output layer\n",
    "fc = (out1 - y)**2 #cost expression\n",
    "\n",
    "\n",
    "cost = theano.function(inputs=[x, y], outputs=fc, updates=[\n",
    "        (theta1, grad_desc(fc, theta1)),\n",
    "        (theta2, grad_desc(fc, theta2))])\n",
    "run_forward = theano.function(inputs=[x], outputs=out1)\n",
    "inputs = np.array([[0,1],[1,0],[1,1],[0,0]]).reshape(4,2) #training data X\n",
    "exp_y = np.array([1, 1, 0, 0]) #training data Y\n",
    "cur_cost = 0\n",
    "for i in range(10000):\n",
    "    for k in range(len(inputs)):\n",
    "        cur_cost = cost(inputs[k], exp_y[k]) #call our Theano-compiled cost function, it will auto update weights\n",
    "    if i % 500 == 0: #only print the cost every 500 epochs/iterations (to save space)\n",
    "        print('Cost: %s' % (cur_cost,))\n",
    "        \n",
    "\n",
    "\n",
    "#Training done! Let's test it out\n",
    "print(run_forward([0,1]))\n",
    "print(run_forward([1,1]))\n",
    "print(run_forward([1,0]))\n",
    "print(run_forward([0,0]))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[GpuElemwise{exp,no_inplace}(<CudaNdarrayType(float32, vector)>), HostFromGpu(GpuElemwise{exp,no_inplace}.0)]\n",
      "Looping 1000 times took 0.332850 seconds\n",
      "Result is [ 1.23178029  1.61879349  1.52278066 ...,  2.20771813  2.29967761\n",
      "  1.62323296]\n",
      "Used the gpu\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using gpu device 0: GeForce GTX 970 (CNMeM is disabled, cuDNN not available)\n"
     ]
    }
   ],
   "source": [
    "from theano import function, config, shared, sandbox\n",
    "import theano.tensor as T\n",
    "import numpy\n",
    "import time\n",
    "\n",
    "vlen = 10 * 30 * 768  # 10 x #cores x # threads per core\n",
    "iters = 1000\n",
    "\n",
    "rng = numpy.random.RandomState(22)\n",
    "x = shared(numpy.asarray(rng.rand(vlen), config.floatX))\n",
    "f = function([], T.exp(x))\n",
    "print(f.maker.fgraph.toposort())\n",
    "t0 = time.time()\n",
    "for i in range(iters):\n",
    "    r = f()\n",
    "t1 = time.time()\n",
    "print(\"Looping %d times took %f seconds\" % (iters, t1 - t0))\n",
    "print(\"Result is %s\" % (r,))\n",
    "if numpy.any([isinstance(x.op, T.Elemwise) for x in f.maker.fgraph.toposort()]):\n",
    "    print('Used the cpu')\n",
    "else:\n",
    "    print('Used the gpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.,  1.,  2.,  1.,  4.], dtype=float32)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = T.fvector()\n",
    "y = x\n",
    "y = T.set_subtensor(y[3], 1)\n",
    "out = y\n",
    "f = theano.function([x], out)\n",
    "f(np.array(range(5)).astype(theano.config.floatX))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ 2,  4,  4,  6,  6, 10, 10,  8,  8, 12]),\n",
       " array([1, 2, 3, 4, 5, 6]),\n",
       " array([  3.,   5.,   8.,   9.,  10.]))"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bs = np.array([0.0,0.0,0.0])\n",
    "ws = np.array([1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0])\n",
    "dEdIns = np.array([1,2,3,4,5,6])\n",
    "prevOut = np.array([2,2,2,2,2])\n",
    "bMult = 2\n",
    "bMultOnes = np.array([1.0,1.0])\n",
    "\n",
    "layerConsols = [[0], [1,2], [3,4], [7,8], [5,6], [9]]\n",
    "flattened = flatten(layerConsols)\n",
    "inverseConsols = [0] * len(flattened)\n",
    "for i in range(len(layerConsols)):\n",
    "    for sub in layerConsols[i]:\n",
    "        inverseConsols[sub] = i\n",
    "\n",
    "inverseConsols = np.array([0,1,1,2,2,4,4,3,3,5])\n",
    "\n",
    "    \n",
    "def gradLayer(nextdEdInputs, layerOutput): #returns (dEdWs, dEdBs, newdEdInputs)\n",
    "        expandedLayerOutput = np.repeat(layerOutput, bMult)\n",
    "        inverseddEdInputs = nextdEdInputs.take(inverseConsols)\n",
    "        dEdWs = expandedLayerOutput * inverseddEdInputs\n",
    "        dEdBs = nextdEdInputs\n",
    "        #upt to here correct\n",
    "        def foo(x):\n",
    "            if x >0.0:\n",
    "                return 1.0\n",
    "            else:\n",
    "                return 0.0\n",
    "        gtCast = np.vectorize(foo)\n",
    "#         dOutsdIns = np.gt(layerOutput, np.zeros_like(layerOutput))\n",
    "#         dOutsdIns = np.cast(dOutsdIns, 'float32')\n",
    "\n",
    "        dOutsdIns = gtCast(layerOutput)\n",
    "        dEdConnections = ws * inverseddEdInputs\n",
    "        dEdConnectionGroups = dEdConnections.reshape((dEdConnections.size // bMult, bMult))\n",
    "        dEdOuts = np.dot(dEdConnectionGroups, bMultOnes) #could need to be axis 1\n",
    "\n",
    "        newdEdInputs = dEdOuts * dOutsdIns\n",
    "\n",
    "        return (dEdWs, dEdBs, newdEdInputs)\n",
    "gradLayer(dEdIns, prevOut)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x = np.array(range(6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 1, 2],\n",
       "       [3, 4, 5]])"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.reshape((2,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numerai\n",
    "trainingfeats, traininglabels, testingfeats, testinglabels = numerai.readTraining()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10000"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(testingfeats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1.], dtype=float32)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "rm evolve.pyc\n",
    "rm fulcrum.pyc\n",
    "rm fractalNetwork2.pyc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'module' object has no attribute 'run'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-31-a940d54d3ba1>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mfulcrum\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mfulcrum\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m: 'module' object has no attribute 'run'"
     ]
    }
   ],
   "source": [
    "import fulcrum\n",
    "fulcrum.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1 == 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
