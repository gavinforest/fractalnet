{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using gpu device 0: GeForce GTX 970\n"
     ]
    }
   ],
   "source": [
    "import mnist\n",
    "import theano\n",
    "import theano.tensor as T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import theano.tensor as T\n",
    "from theano import shared\n",
    "import theano\n",
    "import numpy as np\n",
    "import itertools\n",
    "import typing\n",
    "\n",
    "def flatten(mylist):\n",
    "    return list(itertools.chain.from_iterable(mylist))\n",
    "\n",
    "class net:\n",
    "    weights = []\n",
    "    biases = []\n",
    "    consolidations = []\n",
    "    weightnum = 0\n",
    "    inputdims = 0\n",
    "    \n",
    "    def __init__(self,mylist, inputvecsize, outputvecsize, inputdimension, data):\n",
    "        self.resDenominator = mylist[0]\n",
    "        self.funcs = map(map(deap.compile), mylist[1:])\n",
    "        self.dimensions = len(self.funcs)\n",
    "        self.branchMultiplier = len(flatten(self.funcs))\n",
    "        self.TBranchMult = shared(np.array([self.branchMultiplier]))\n",
    "        self.dataset = [shared(datum.astype(theano.config.floatX)) for (datum, outp) in data]\n",
    "        self.datalabels = [shared(outp) for (datum, outp) in data]\n",
    "        self.dataSample = data[0][0] #numpy\n",
    "        self.inputdimension = inputdimension\n",
    "        self.genweights(inputvecsize, outputvecsize, inputdimension)\n",
    "\n",
    "    def applyFuncs(self, narray): #checked\n",
    "        return [np.add.at(narray, [dim], f(narray[dim]))\n",
    "                for f in self.funcs[dim] for dim in range(self.dimensions)]\n",
    "\n",
    "    def applyFuncsMult(self,narrays): \n",
    "        return flatten([self.applyFuncs(x) for x in narrays])\n",
    "        \n",
    "    def genConsolidate(self): #finds consolidation list from located self.dataSample\n",
    "        indices = []\n",
    "        \n",
    "        def zeroEnd(thing):\n",
    "            thing[-1] = 0.0\n",
    "            return thing\n",
    "            \n",
    "        located = self.locate(self.dataSample)\n",
    "        \n",
    "        myArrays = [zeroEnd(array) for array in located] #only location matters\n",
    "\n",
    "        for i in range(len(myArrays)):\n",
    "            if [x for x in flatten(indices) if x == i] == []:\n",
    "                subgroup = [i] + [j for j in range(i + 1, len(myArrays))\n",
    "                                    if myArrays[i] == myArrays[j]]\n",
    "    \n",
    "                indices.append(subgroup)\n",
    "        #NUMPY ARRAY, NOT THEANO\n",
    "        return np.array(indices) #each sublist contains the element to which other elements should be added\n",
    "                                #and the elements to add to it. If no overlap occurs, the sublist is just 1 element long\n",
    "                                #before creating a sublist of an element, it checks to make sure the element is not already \n",
    "                                #part of indices\n",
    "        \n",
    "    def consolidateTensor(tensor, consolidationList):\n",
    "        for sublist in consolidationList:\n",
    "            for i in range(T.prod(sublist.shape),0,-1):  #iterate from second to last element backwards\n",
    "                #increment tensor specified by sublist[0] at position [-1] by the [-1] element of the other tensors in sublist\n",
    "                tensor.inc_subtensor(tensors[sublist[0]][-1], tensors[sublist[i]][-1]) #use last element = activation\n",
    "                del tensors[sublist[i]] #delete tensor that was added to sublist[0] tensor\n",
    "        \n",
    "        return tensor\n",
    "        \n",
    "    def consolidate(tensors, consolidationList): #for theano\n",
    "        for sublist in consolidationList:\n",
    "            for i in sublist[1:]:  #iterate from second to last element backwards\n",
    "                #increment tensor specified by sublist[0] at position [-1] by the [-1] element of the other tensors in sublist\n",
    "                tensors[sublist[0]].inc_subtensor(tensors[sublist[0]][-1], tensors[sublist[i]][-1]) #use last element = activation\n",
    "                del tensors[sublist[i]] #delete tensor that was added to sublist[0] tensor\n",
    "        \n",
    "        return tensors\n",
    "        \n",
    "    def numpyConsolidate(narrays, consolidationList): #for numpy\n",
    "        for sublist in consolidationList:\n",
    "            for i in range(np.prod(sublist.shape),0,-1):  #iterate from second to last element backwards\n",
    "                np.add.at(narrays[sublist[0]], [-1], narrays[sublist[i]][-1]) #use last element = activation\n",
    "                del narrays[sublist[i]]\n",
    "        \n",
    "        return narrays\n",
    "        \n",
    "        #this is only needed when generating consolidation arrays. Otherwise, can just use elements without spacial coordinates\n",
    "        \n",
    "    def locate(self, sample):  #only 1 or 2 implemented\n",
    "        insize = sample.size\n",
    "        sample = np.ravel(sample)\n",
    "        tensorFrame = [0] * (self.dimensions + 1)\n",
    "\n",
    "        located = []\n",
    "        #self.inputdimension is the dimension the input should be represented in\n",
    "        if self.inputdimension == 1:\n",
    "            for i in range(insize):\n",
    "                myTens = tensorframe\n",
    "                myTens[0] = i\n",
    "                myTens[-1] = sample[i]\n",
    "                located.append(myTens)\n",
    "        \n",
    "        elif self.inputdimension == 2:\n",
    "            if not self.inputdims:\n",
    "                factors = ((i, insize/i) for i in range(int(insize**0.5),0,-1)\n",
    "                            if insize % i == 0)\n",
    "                self.inputdims = factors.next()\n",
    "                \n",
    "            x,y = self.inputdims\n",
    "            for i in range(x):\n",
    "                for j in range(y):\n",
    "                    myTens = tensorframe\n",
    "                    myTens[0] = i\n",
    "                    myTens[1] = j\n",
    "                    myTens[-1] = sample[i * x + j]\n",
    "                    indexed.append(myTens)\n",
    "        \n",
    "        return located #returns list of numpy arrays in same order as sample\n",
    "    \n",
    "    def genweights(self, indimension, sampleinput, outsize, threshold):\n",
    "        myArrays = self.locate(sampleinput)\n",
    "        myWeightNum = 0\n",
    "        self.hiddenlayers = 0\n",
    "        while myWeightNum < threshold:\n",
    "            new = self.applyFuncsMult(myArrays)\n",
    "            \n",
    "            weightVec = shared(np.random.rand(len(new)))\n",
    "            biasVec   = shared(np.random.rand(len(new)))\n",
    "            self.weights.append(weightVec)\n",
    "            self.biases.append(biasVec)\n",
    "            myWeightNum += len(new)\n",
    "            \n",
    "            self.consolidations.append(genConsolidate(new))\n",
    "            myArrays = numpyConsolidate(new, self.consolidations[-1])\n",
    "            \n",
    "            #convert to shared data type for later speed\n",
    "            self.consolidations[-1] = [shared(subArray) for subArray in self.consolidations[-1]] \n",
    "            self.hiddenlayers += 1\n",
    "        \n",
    "        self.weights.append(shared(\n",
    "            np.random.rand(len(mytensors),outsize)))        #final interconnected layer for output\n",
    "        self.biases.append(shared(\n",
    "            np.random.rand(len(mytensors))))\n",
    "        \n",
    "        myWeightNum += len(mytensors) * outsize\n",
    "        self.weightNum = myWeightNum\n",
    "            \n",
    "    def feedForward(self, inp, weights, biases, consolidations, hiddenlayers):\n",
    "        inputs = T.ravel(inp)\n",
    "        \n",
    "        for i in range(hiddenlayers):\n",
    "            new = \n",
    "            new = new * weights[i] + biases[i]\n",
    "            new = self.consolidate(new, consolidations[i])\n",
    "            new = [T.tanh(tensor) for tensor in new]\n",
    "            inputs = new\n",
    "        \n",
    "        inputs = theano.dot(inputs, self.weights[-1])\n",
    "        inputs += self.biases[-1]\n",
    "        return T.tanh(inputs)\n",
    "        \n",
    "    def error(self,inp, outp):\n",
    "        diff = outp - self.feedForward(inp)\n",
    "        return T.sum(T.dot(diff, diff))\n",
    "        \n",
    "    def train(self, num_epochs):\n",
    "\n",
    "        ind = T.iscalar('index')\n",
    "        x = T.tensor('input')\n",
    "        y = T.tensor('output')\n",
    "        err = T.scalar(self.error(x,y))\n",
    "        error = theano.function([x,y], err)\n",
    "        \n",
    "        gw = [theano.grad(error,weight) for weight in self.weights]\n",
    "        gb = [theano.grad(error,bias)   for bias in self.biases]\n",
    "        updates = [(weight, weight - wgrad * alpha) for weight, wgrad in zip(self.weights, gw)] +\\\n",
    "                  [(bias  , bias  -  bgrad * alpha) for bias ,  bgrad in zip(self.biases , gb)]\n",
    "        train = theano.function(\n",
    "                                inputs = [ind],\n",
    "                                outputs = err,\n",
    "                                updates = updates,\n",
    "                                given = {\n",
    "                                    x : self.dataset[ind],\n",
    "                                    y : self.datalabels[ind] }\n",
    "                                )\n",
    "        print '... training'\n",
    "        for i in range(num_epochs):\n",
    "            errors = []\n",
    "            for ind in range(len(self.datalabels)):\n",
    "                errors.append(train(ind))\n",
    "            print \"epoch \" + str(i) + \" complete, error is \" + sum(errors)/len(self.datalabels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import operator\n",
    "import math\n",
    "from deap import gp, creator, base, tools, algorithms\n",
    "from random import randint, uniform\n",
    "import random\n",
    "import pickle\n",
    "import itertools\n",
    "import time\n",
    "import math\n",
    "from itertools import izip\n",
    "import numpy as np\n",
    "\n",
    "constantLims = (-5.0, 5.0)\n",
    "resolutionLimits = (30,50)\n",
    "resModifyLimits = (-5,5)\n",
    "resModifyRange = (0,5)\n",
    "dimAddingRange = (0,5)\n",
    "alphaModifyRange = (0,5)\n",
    "alphaModifyMult = (0.5,1.5)\n",
    "\n",
    "\n",
    "DataDims = 1\n",
    "MINDIMFUNCS = 8\n",
    "MAXDIMFUNCS = 40\n",
    "MINDIMS = DataDims\n",
    "MAXDIMS = 5\n",
    "MINFUNCDEPTH = 2\n",
    "MAXFUNCDEPTH = 5\n",
    "CXPB = 0.7\n",
    "MUTPB = 0.3\n",
    "MUTSTRENGTH = 0.3\n",
    "FitnessMaximize = -1.0\n",
    "defaultAlpha = 0.002\n",
    "\n",
    "unif = lambda: uniform(*constantLims)\n",
    "\n",
    "def square(x):\n",
    "    return x*x\n",
    "def absSqrt(x):\n",
    "    return math.sqrt(abs(x))\n",
    "    \n",
    "singletons = [math.sin, math.cos, square, absSqrt]\n",
    "doubles = [operator.mul, operator.add, operator.sub]\n",
    "\n",
    "class Tree:\n",
    "    def __init__(self, depth):\n",
    "        self.depth = depth\n",
    "        \n",
    "        if depth > 0: #going to branch\n",
    "            if random.choice([True, False]):\n",
    "                self.single = True\n",
    "                self.op = random.choice(singletons)\n",
    "                self.below = Tree(self.depth - 1)\n",
    "            else:\n",
    "                self.single = False\n",
    "                self.op = random.choice(doubles)\n",
    "                self.left = Tree(self.depth -1)\n",
    "                self.right = Tree(self.depth - 1)\n",
    "\n",
    "        else: #going to be a constant or a variable\n",
    "            if random.choice([True, False]):\n",
    "                self.isConstant= True\n",
    "                self.constant = unif()\n",
    "            else:\n",
    "                self.isConstant = False\n",
    "            \n",
    "    \n",
    "    def __call__(self, x):\n",
    "        if self.depth == 0:\n",
    "            if self.isConstant:\n",
    "                return self.constant\n",
    "            else:\n",
    "                return x\n",
    "        elif self.single:\n",
    "            return self.op(self.below(x))\n",
    "        else:\n",
    "            return self.op(self.left(x), self.right(x))\n",
    "            \n",
    "\n",
    "\n",
    "\n",
    "class Individual:\n",
    "    def __init__(self):\n",
    "        frameLength = [None] * random.randint(MINDIMS, MAXDIMS)\n",
    "        lengthDimFuncs = [random.randint(MINDIMFUNCS, MAXDIMFUNCS) for i in frameLength]\n",
    "        funcDepths = [[random.randint(MINFUNCDEPTH, MAXFUNCDEPTH) for i in range(j)] for j in lengthDimFuncs]\n",
    "        self.funcs = [map(Tree, l) for l in funcDepths]\n",
    "        self.resolution = random.randint(*resolutionLimits)\n",
    "        self.alpha = defaultAlpha\n",
    "        \n",
    "        self.fitness = None\n",
    "        self.fitnessValid = False\n",
    "        \n",
    "    def mutate(self):\n",
    "        willAdd = random.choice([1,0,-1])\n",
    "        willModDim = random.choice([True, False])\n",
    "        mutatedFuncs  = []\n",
    "        mutatedAlpha = self.alpha\n",
    "        mutatedRes = self.resolution\n",
    "\n",
    "        #mutating functions\n",
    "        for funcList in self.funcs:\n",
    "            mutatedFuncList = []\n",
    "            for f in funcList:\n",
    "                if random.random() < MUTSTRENGTH:\n",
    "                    mutatedFuncList.append(Tree(random.randint(MINFUNCDEPTH, MAXFUNCDEPTH)))\n",
    "                else:\n",
    "                    mutatedFuncList.append(f)\n",
    "            mutatedFuncs.append(mutatedFuncList)\n",
    "\n",
    "        if willAdd == 1:\n",
    "            if willModDim:\n",
    "                mutatedFuncs = mutatedFuncs + [[Tree(random.randint(MINFUNCDEPTH, MAXFUNCDEPTH)) for i in range(random.randint(MINDIMFUNCS, MAXDIMFUNCS))]]\n",
    "            else:\n",
    "                ind = random.randint(0, len(mutatedFuncs)-1)\n",
    "                mutatedFuncs[ind] = mutatedFuncs[ind] + [Tree(random.randint(MINFUNCDEPTH, MAXFUNCDEPTH))]\n",
    "        elif willAdd == -1:\n",
    "            if willModDim:\n",
    "                mutatedFuncs.pop(random.randint(0, len(mutatedFuncs)-1))\n",
    "            else:\n",
    "                ind = random.randint(0, len(mutatedFuncs) - 1)\n",
    "                mutatedFuncs[ind].pop(random.randint(0, len(mutatedFuncs[ind]) - 1))\n",
    "\n",
    "        willMutateAlpha = random.uniform(*alphaModifyRange) < 1.0\n",
    "        if willMutateAlpha:\n",
    "            mutatedAlpha *= random.uniform(*alphaModifyMult)\n",
    "\n",
    "        willMutateRes = random.uniform(*resModifyRange) < 1.0\n",
    "        if willMutateRes:\n",
    "            mutatedRes += random.randint(*resModifyLimits)\n",
    "            if mutatedRes < resolutionLimits[0]:\n",
    "                mutatedRes = resolutionLimits[0]\n",
    "            elif mutatedRes > resolutionLimits[1]:\n",
    "                mutatedRes = resolutionLimits[1]\n",
    "\n",
    "\n",
    "        self.funcs = mutatedFuncs\n",
    "        self.alpha = mutatedAlpha\n",
    "        self.resolution = mutatedRes\n",
    "        \n",
    "        return self\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-1.4537320932844144, -0.4161468365471424, 3.650479242518307, 23.574035388945003, 1.4663457891000569, 2.0363678492468256, 56.48535438309808, 95.3232231407075, 0.7641404871774221, 535.7339624979033]\n"
     ]
    }
   ],
   "source": [
    "thingList = [Tree(x) for x in range(10)]\n",
    "results = [thing(2) for thing in thingList]\n",
    "print results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[[<__main__.Tree instance at 0x38ecab8>,\n",
       "   <__main__.Tree instance at 0x37e3560>,\n",
       "   <__main__.Tree instance at 0x37e18c0>,\n",
       "   <__main__.Tree instance at 0x37dfef0>,\n",
       "   <__main__.Tree instance at 0x3919c20>,\n",
       "   <__main__.Tree instance at 0x3919fc8>,\n",
       "   <__main__.Tree instance at 0x38f5b00>,\n",
       "   <__main__.Tree instance at 0x391a290>,\n",
       "   <__main__.Tree instance at 0x38f5638>,\n",
       "   <__main__.Tree instance at 0x38f5b48>,\n",
       "   <__main__.Tree instance at 0x391a680>,\n",
       "   <__main__.Tree instance at 0x39011b8>,\n",
       "   <__main__.Tree instance at 0x3901488>,\n",
       "   <__main__.Tree instance at 0x391aa28>,\n",
       "   <__main__.Tree instance at 0x391abd8>,\n",
       "   <__main__.Tree instance at 0x38feef0>,\n",
       "   <__main__.Tree instance at 0x38ff290>,\n",
       "   <__main__.Tree instance at 0x38ff5f0>,\n",
       "   <__main__.Tree instance at 0x38ff7a0>],\n",
       "  [<__main__.Tree instance at 0x391add0>,\n",
       "   <__main__.Tree instance at 0x3905cb0>,\n",
       "   <__main__.Tree instance at 0x3905ab8>,\n",
       "   <__main__.Tree instance at 0x39055f0>,\n",
       "   <__main__.Tree instance at 0x391af80>,\n",
       "   <__main__.Tree instance at 0x391b170>,\n",
       "   <__main__.Tree instance at 0x3902d88>,\n",
       "   <__main__.Tree instance at 0x391ba70>,\n",
       "   <__main__.Tree instance at 0x3902518>,\n",
       "   <__main__.Tree instance at 0x3902f38>,\n",
       "   <__main__.Tree instance at 0x391bb90>,\n",
       "   <__main__.Tree instance at 0x391c050>,\n",
       "   <__main__.Tree instance at 0x3904050>,\n",
       "   <__main__.Tree instance at 0x391c2d8>,\n",
       "   <__main__.Tree instance at 0x3900710>,\n",
       "   <__main__.Tree instance at 0x3900b48>,\n",
       "   <__main__.Tree instance at 0x3906cb0>,\n",
       "   <__main__.Tree instance at 0x3906e18>],\n",
       "  [<__main__.Tree instance at 0x391cab8>,\n",
       "   <__main__.Tree instance at 0x390a710>,\n",
       "   <__main__.Tree instance at 0x391ccb0>,\n",
       "   <__main__.Tree instance at 0x390add0>,\n",
       "   <__main__.Tree instance at 0x3909050>,\n",
       "   <__main__.Tree instance at 0x3909dd0>,\n",
       "   <__main__.Tree instance at 0x3907248>,\n",
       "   <__main__.Tree instance at 0x39074d0>,\n",
       "   <__main__.Tree instance at 0x3907878>,\n",
       "   <__main__.Tree instance at 0x391d050>,\n",
       "   <__main__.Tree instance at 0x390b488>,\n",
       "   <__main__.Tree instance at 0x390b680>,\n",
       "   <__main__.Tree instance at 0x390b878>,\n",
       "   <__main__.Tree instance at 0x390bc20>,\n",
       "   <__main__.Tree instance at 0x390c368>,\n",
       "   <__main__.Tree instance at 0x390c488>,\n",
       "   <__main__.Tree instance at 0x390c830>,\n",
       "   <__main__.Tree instance at 0x390d0e0>,\n",
       "   <__main__.Tree instance at 0x390d2d8>,\n",
       "   <__main__.Tree instance at 0x390d6c8>,\n",
       "   <__main__.Tree instance at 0x390dd40>,\n",
       "   <__main__.Tree instance at 0x390e0e0>,\n",
       "   <__main__.Tree instance at 0x391d830>,\n",
       "   <__main__.Tree instance at 0x391d950>],\n",
       "  [<__main__.Tree instance at 0x390e710>,\n",
       "   <__main__.Tree instance at 0x390e9e0>,\n",
       "   <__main__.Tree instance at 0x390f098>,\n",
       "   <__main__.Tree instance at 0x391dbd8>,\n",
       "   <__main__.Tree instance at 0x390f830>,\n",
       "   <__main__.Tree instance at 0x390f950>,\n",
       "   <__main__.Tree instance at 0x390fcb0>,\n",
       "   <__main__.Tree instance at 0x3910320>,\n",
       "   <__main__.Tree instance at 0x391e0e0>,\n",
       "   <__main__.Tree instance at 0x3910b90>,\n",
       "   <__main__.Tree instance at 0x391e248>,\n",
       "   <__main__.Tree instance at 0x3910d88>,\n",
       "   <__main__.Tree instance at 0x3910ef0>,\n",
       "   <__main__.Tree instance at 0x3911290>,\n",
       "   <__main__.Tree instance at 0x39113b0>,\n",
       "   <__main__.Tree instance at 0x3911d40>,\n",
       "   <__main__.Tree instance at 0x3911f80>,\n",
       "   <__main__.Tree instance at 0x391e488>,\n",
       "   <__main__.Tree instance at 0x391e5f0>,\n",
       "   <__main__.Tree instance at 0x3912560>,\n",
       "   <__main__.Tree instance at 0x3912a70>,\n",
       "   <__main__.Tree instance at 0x39132d8>,\n",
       "   <__main__.Tree instance at 0x39133f8>,\n",
       "   <__main__.Tree instance at 0x3913cf8>,\n",
       "   <__main__.Tree instance at 0x391e7e8>,\n",
       "   <__main__.Tree instance at 0x3913f38>,\n",
       "   <__main__.Tree instance at 0x3914170>,\n",
       "   <__main__.Tree instance at 0x39147a0>,\n",
       "   <__main__.Tree instance at 0x391e998>,\n",
       "   <__main__.Tree instance at 0x391eb90>],\n",
       "  [<__main__.Tree instance at 0x39157e8>,\n",
       "   <__main__.Tree instance at 0x3915ab8>,\n",
       "   <__main__.Tree instance at 0x3915bd8>,\n",
       "   <__main__.Tree instance at 0x3916128>,\n",
       "   <__main__.Tree instance at 0x391ee18>,\n",
       "   <__main__.Tree instance at 0x391efc8>,\n",
       "   <__main__.Tree instance at 0x391f128>,\n",
       "   <__main__.Tree instance at 0x3917050>,\n",
       "   <__main__.Tree instance at 0x39172d8>,\n",
       "   <__main__.Tree instance at 0x391f2d8>,\n",
       "   <__main__.Tree instance at 0x39179e0>,\n",
       "   <__main__.Tree instance at 0x3917d88>,\n",
       "   <__main__.Tree instance at 0x3917f80>,\n",
       "   <__main__.Tree instance at 0x39180e0>,\n",
       "   <__main__.Tree instance at 0x391f3f8>,\n",
       "   <__main__.Tree instance at 0x39182d8>,\n",
       "   <__main__.Tree instance at 0x3918440>,\n",
       "   <__main__.Tree instance at 0x3918710>,\n",
       "   <__main__.Tree instance at 0x3918cf8>,\n",
       "   <__main__.Tree instance at 0x391f518>,\n",
       "   <__main__.Tree instance at 0x391f758>,\n",
       "   <__main__.Tree instance at 0x39190e0>,\n",
       "   <__main__.Tree instance at 0x3919290>,\n",
       "   <__main__.Tree instance at 0x391f908>,\n",
       "   <__main__.Tree instance at 0x3919a28>]]]"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = [Individual().mutate().funcs for i in range(1)]\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[] + [None] * 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([3, 5, 7])"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "np.arange(6).reshape((2,3)).sum(axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading local directories\n",
      "Reading remote server file list\n",
      "Synchronizing files\n",
      "sync \"./Skynet/grow.ipynb\" changed in local. uploading\n",
      "Error syncing \"./Skynet/grow.ipynb\": HTTP 412\n",
      "sync \"./Skynet/Untitled1.ipynb\" changed in local. uploading\n",
      "Error syncing \"./Skynet/Untitled1.ipynb\": HTTP 412\n",
      "sync \"./Skynet/fractalNetwork2.py\" changed in remote. downloading\n",
      "sync \"./Skynet/fractalNetwork2.pyc\" changed in local. uploading\n",
      "Finished!\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "cd ..\n",
    "grive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
