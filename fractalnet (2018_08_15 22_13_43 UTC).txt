import theano.tensor as T
from theano import shared
import theano
import numpy as np
import itertools
import typing

def flatten(mylist):
    return list(itertools.chain.from_iterable(mylist))

class net:
    weights = []
    biases = []
    consolidations = []
    weightnum = 0
    inputdims = 0
    
    def __init__(self,mylist, inputvecsize, outputvecsize, inputdimension, data):
        self.resDenominator = mylist[0]
        self.funcs = mylist[1:]
        self.dimensions = len(self.funcs)
        self.branchMultiplier = len(flatten(self.funcs))
        self.dataset = [shared(datum.astype(theano.config.floatX)) for (datum, outp) in data]
        self.datalabels = [shared(outp) for (datum, outp) in data]
        self.dataSample = data[0]
        self.genweights(inputvecsize, outputvecsize, inputdimension)
        self.inputdimension = inputdimension
    
    def applyFuncs(self, tensor): #checked
        def incArray(index, increment)
        return [tensor.inc_subtensor(tensor[dim], f(tensor[dim])) 
                for f in self.funcs[dim] for dim in range(self.dimensions)]

    def applyFuncsMult(self,tensors): 
        return flatten([self.applyFuncs(x) for x in tensors])
        
    def genConsolidate(self): #finds consolidation list from located self.dataSample
        indices = []
        
        def setEnd(thing):
            thing[-1] = 0.0
            return thing
            
        located = self.locate(self.dataSample)
        
        mytensors = [setEnd(array) for array in located] #only location matters

        for i in range(len(mytensors)):
            #sees if tensors have same location, consolidates if they do
            subgroup = [i] + [j for j in range(i + 1, len(mytensors))
                                if mytensors[i] == mytensors[j]] #don't want to delete yourself!
            
            indices.append(shared(np.array(subgroup)))
            for index in subgroup[::-1]:    #iterate backwards so deletion does not change previous indices
                del mytensors[index]
        
        return indices #each sublist contains the element to which other elements should be added
                        #and the elements to add to it. They are then deleted and iteration continues
        
    def consolidate(tensors, consolidationList): 
        for sublist in consolidationList:
            for i in range(T.prod(sublist.shape),0,-1):  #iterate from second to last element backwards
                tensors[sublist[0]].inc_subtensor(tensors[sublist[0]][-1], tensors[sublist[i]][-1]) #use last element = activation
                del tensors[i]
        
        return tensors
        
    def locate(self, sample):  #only 1 or 2 implemented
        insize = sample.size
        sample = np.ravel(sample)
        tensorFrame = [0] * (self.dimensions + 1)

        located = []
        #self.inputdimension is the dimension the input should be represented in
        if self.inputdimension == 1:
            for i in range(insize):
                myTens = tensorframe
                myTens[0] = i
                myTens[-1] = sample[i]
                located.append(myTens)
        
        elif self.inputdimension == 2:
            if not self.inputdims:
                factors = ((i, insize/i) for i in range(int(insize**0.5),0,-1)
                            if insize % i == 0)
                self.inputdims = factors.next()
                
            x,y = self.inputdims
            for i in range(x):
                for j in range(y):
                    myTens = tensorframe
                    myTens[0] = i
                    myTens[1] = j
                    myTens[-1] = sample[i * x + j]
                    indexed.append(myTens)
        
        return [shared(thing) for thing in located]

        
        
    
    def genweights(self, indimension, sampleinput, outsize, threshold):
        mytensors = self.locate(sampleinput)
        myWeightNum = 0
        self.hiddenlayers = 0
        while myWeightNum < threshold:
            new = self.applyFuncsMult(mytensors)
            
            weightVec = shared(np.random.rand(len(new)))
            biasVec   = shared(np.random.rand(len(new)))
            self.weights.append(weightVec)
            self.biases.append(biasVec)
            myWeightNum += len(new)
            
            self.consolidations.append(genConsolidate(new))
            mytensors = consolidate(new, self.consolidations[-1])
            self.hiddenlayers += 1
        
        self.weights.append(shared(
            np.random.rand(len(mytensors),outsize)))        #final interconnected layer for output
        self.biases.append(shared(
            np.random.rand(len(mytensors))))
        
        myWeightNum += len(mytensors) * outsize
        self.weightNum = myWeightNum
            
            
    def feedForward(self, inp):
        inputs = self.locate(inp)
        
        for i in range(self.hiddenlayers):
            new = self.applyFuncsMult(inputs)
            new = [tensor.set_subtensor(tensor[-1], tensor[-1] * w + b) 
                    for tensor, w, b in zip(new, self.weights[i], self.biases[i])]
            new = self.consolidate(new, self.consolidations[i])
            new = [T.tanh(tensor) for tensor in new]
            inputs = new
        
        inputs = shared(np.array([tensor[-1] for tensor in inputs]))
        inputs = theano.dot(inputs, self.weights[-1])
        inputs += self.biases[-1]
        return T.tanh(inputs)
        
        
    def error(self,inp, outp):
        diff = outp - self.feedForward(inp)
        return T.sum(T.dot(diff, diff))
        
    def train(self, num_epochs):

        ind = T.iscalar('index')
        x = T.tensor('input')
        y = T.tensor('output')
        err = T.scalar(self.error(x,y))
        error = theano.function([x,y], err)
        
        gw = [theano.grad(error,weight) for weight in self.weights]
        gb = [theano.grad(error,bias)   for bias in self.biases]
        updates = [(weight, weight - wgrad * alpha) for weight, wgrad in zip(self.weights, gw)] +\
                  [(bias  , bias  -  bgrad * alpha) for bias ,  bgrad in zip(self.biases , gb)]
        train = theano.function(
                                inputs = [ind],
                                outputs = err,
                                updates = updates,
                                given = {
                                    x : self.dataset[ind],
                                    y : self.datalabels[ind] }
                                )
        print '... training'
        for i in range(num_epochs):
            errors = []
            for ind in range(len(self.datalabels)):
                errors.append(train(ind))
            print "epoch " + str(i) + " complete, error is " + sum(errors)/len(self.datalabels)
            
        